% Chapter: Evaluation Methodology
\chapter{Evaluation Methodology}
\label{ch:evaluation}

This chapter presents the research methodology employed to evaluate the ontology-driven code generation framework. We articulate specific research questions, formulate testable hypotheses, describe the evaluation methodology including study design and metrics, compare against baseline approaches, and detail the experimental infrastructure.

\section{Research Questions}
\label{sec:research-questions}

The evaluation is structured around five research questions (RQs) that collectively assess the feasibility, effectiveness, and scalability of ontology-driven code generation for API contract management.

\subsection{RQ1: Single Source of Truth}
\label{subsec:rq1}

\textbf{Research Question 1:} \emph{Can RDF ontologies serve as a single source of truth for generating multiple consistent code artifacts across the full software stack?}

\paragraph{Motivation.} Traditional API development maintains separate artifacts (OpenAPI specifications, type definitions, validation schemas, documentation) that drift out of synchronization. This fragmentation leads to contract violations, runtime failures, and maintenance overhead. If RDF ontologies can serve as a unified semantic model from which all artifacts derive, the synchronization problem vanishes by construction.

\paragraph{Measurable Objectives.} We seek to demonstrate that:
\begin{enumerate}
    \item Multiple heterogeneous artifacts (OpenAPI YAML, TypeScript types, Zod schemas, JSDoc documentation) can be generated from a single RDF ontology
    \item Generated artifacts remain synchronized regardless of ontology complexity
    \item Changes to the ontology propagate consistently to all dependent artifacts
\end{enumerate}

\paragraph{Success Criteria.} RQ1 is considered validated if:
\begin{itemize}
    \item 100\% of entities defined in the ontology appear in all generated artifacts
    \item 100\% of property constraints (type, required, pattern, bounds) are represented consistently across artifacts
    \item Zero semantic conflicts exist between generated artifacts (e.g., a required field in the ontology is required in all derived schemas)
    \item Ontology modifications trigger complete re-generation with maintained consistency
\end{itemize}

\subsection{RQ2: Code Maintainability}
\label{subsec:rq2}

\textbf{Research Question 2:} \emph{What is the impact of ontology-driven generation on code maintainability compared to hand-written implementations?}

\paragraph{Motivation.} Maintainability encompasses several dimensions: time to implement changes, risk of introducing defects, effort required for refactoring, and cognitive load on developers. Generated code is sometimes criticized as verbose, difficult to debug, or brittle when requirements change. Conversely, if generation is deterministic and ontology-centric, modifications should be simpler and safer.

\paragraph{Measurable Objectives.} We compare maintainability across several dimensions:
\begin{enumerate}
    \item \textbf{Time-to-change}: Duration to add a new entity or modify existing properties
    \item \textbf{Change propagation}: Number of files requiring manual edits per logical change
    \item \textbf{Defect introduction}: Likelihood of introducing contract violations during evolution
    \item \textbf{Code volume}: Lines of code that developers must understand and maintain
\end{enumerate}

\paragraph{Success Criteria.} The ontology-driven approach demonstrates superior maintainability if:
\begin{itemize}
    \item Time-to-change is reduced by at least 40\% compared to manual approaches
    \item Change propagation requires editing only the ontology (1 file) vs. 5+ files in traditional approaches
    \item Zero contract violations are introduced during ontology evolution (vs. 5-10\% in manual approaches per \cite{api-evolution})
    \item Developer-maintained code volume decreases by 60-80\% (ontology + templates vs. all manual artifacts)
\end{itemize}

\subsection{RQ3: Deterministic Reproducibility}
\label{subsec:rq3}

\textbf{Research Question 3:} \emph{How does the deterministic generation guarantee reproducibility across different systems, developers, and time?}

\paragraph{Motivation.} Non-deterministic generation undermines trust and complicates version control. If the same ontology produces different artifacts on different machines or at different times, developers cannot reliably review changes, and continuous integration systems may flag spurious differences. Deterministic generation is a prerequisite for reproducible builds and auditable artifacts.

\paragraph{Measurable Objectives.} We evaluate determinism at multiple levels:
\begin{enumerate}
    \item \textbf{Byte-identical outputs}: Given identical inputs (ontology, templates, queries), outputs are bit-for-bit identical
    \item \textbf{Platform independence}: Outputs are identical across operating systems (Linux, macOS, Windows)
    \item \textbf{Temporal stability}: Outputs remain identical across repeated executions and time intervals
    \item \textbf{Toolchain independence}: Different versions of the Rust toolchain produce identical outputs
\end{enumerate}

\paragraph{Success Criteria.} Determinism is validated if:
\begin{itemize}
    \item SHA-256 hash of generated artifacts is identical across 100 repeated runs
    \item Cross-platform generation (Linux/macOS/Windows) produces identical outputs
    \item Generation at time $t$ and $t+30$ days produces identical outputs for unchanged ontology
    \item Rust toolchain versions (within same major release) produce identical outputs
\end{itemize}

\subsection{RQ4: Enterprise Scalability}
\label{subsec:rq4}

\textbf{Research Question 4:} \emph{Can the RDF-based approach scale to enterprise-level API specifications with 1000+ endpoints and complex domain models?}

\paragraph{Motivation.} Small examples validate technical feasibility but do not establish practical viability. Enterprise APIs routinely exceed 1000 endpoints with deeply nested entity hierarchies, complex validation rules, and extensive cross-references. If SPARQL query performance degrades non-linearly or template rendering becomes prohibitively slow, the approach fails at realistic scales.

\paragraph{Measurable Objectives.} We assess scalability across several dimensions:
\begin{enumerate}
    \item \textbf{Generation latency}: Time to generate all artifacts as ontology size increases
    \item \textbf{Memory consumption}: Peak memory usage during generation
    \item \textbf{Query performance}: SPARQL query execution time vs. triple count
    \item \textbf{Artifact size}: Generated code size and compilation time
\end{enumerate}

\paragraph{Success Criteria.} The approach is considered scalable if:
\begin{itemize}
    \item Generation completes in <100ms for ontologies with 5000 triples (typical 100-endpoint API)
    \item Generation completes in <500ms for ontologies with 50,000 triples (typical 1000-endpoint API)
    \item Memory consumption remains <500MB for realistic enterprise ontologies
    \item SPARQL query performance scales sub-linearly (O(n log n) or better) with triple count
    \item Generated TypeScript code compiles in <10s for 1000-endpoint specifications
\end{itemize}

\subsection{RQ5: Developer Cognitive Load}
\label{subsec:rq5}

\textbf{Research Question 5:} \emph{What cognitive load reduction do type guards, validation schemas, and synchronization guarantees provide to developers?}

\paragraph{Motivation.} Cognitive load encompasses the mental effort required to understand, modify, and debug code. If developers must mentally track consistency across multiple artifact formats, verify type safety manually, or reason about complex validation logic, cognitive load increases. Conversely, if the framework provides automatic synchronization and type-safe interfaces, developers can focus on business logic.

\paragraph{Measurable Objectives.} We measure cognitive load through:
\begin{enumerate}
    \item \textbf{Mental model complexity}: Number of distinct artifact formats developers must understand
    \item \textbf{Error detection latency}: Time from contract violation introduction to detection (compile-time vs. runtime)
    \item \textbf{Debugging effort}: Time to locate and fix type-related defects
    \item \textbf{Onboarding time}: Time for new developers to become productive
\end{enumerate}

\paragraph{Success Criteria.} Cognitive load is reduced if:
\begin{itemize}
    \item Developers interact with 1 format (RDF ontology) vs. 5+ formats (OpenAPI, TypeScript, JSON Schema, etc.)
    \item 80\%+ of contract violations are detected at compile-time (immediate feedback) vs. runtime discovery
    \item Time to fix type-related defects decreases by 50\%+ due to precise error messages and type guards
    \item New developer onboarding time decreases by 30\%+ due to single source of truth
\end{itemize}

\section{Research Hypotheses}
\label{sec:hypotheses}

Building on the research questions, we formulate four testable hypotheses with specific quantitative predictions.

\subsection{H1: Specification-Implementation Gap}
\label{subsec:h1}

\textbf{Hypothesis 1:} \emph{RDF-based specification reduces the specification-to-implementation gap to <1\%, compared to 5-10\% for traditional manual approaches.}

\paragraph{Rationale.} Manual maintenance of multiple synchronized artifacts is error-prone. Studies of API evolution \cite{api-evolution} document specification-implementation drift rates of 5-10\% within three months of initial development. Automated generation from a single source eliminates manual synchronization, reducing drift to measurement error levels.

\paragraph{Operationalization.} We define specification-implementation gap as:
\begin{equation}
\text{Gap} = \frac{\text{Detected Contract Violations}}{\text{Total Contract Assertions}} \times 100\%
\end{equation}

Contract violations include: missing required fields, incorrect types, violated constraints, undocumented endpoints.

\paragraph{Validation Approach.} We compare:
\begin{itemize}
    \item Ontology-driven approach: Generate artifacts, run comprehensive test suite detecting contract violations
    \item Manual approach: Hand-write equivalent artifacts, run identical test suite
    \item Measure gap after initial development and after 5 change cycles
\end{itemize}

\subsection{H2: Type Safety and Defect Reduction}
\label{subsec:h2}

\textbf{Hypothesis 2:} \emph{Type guard composition reduces type-related runtime defects by 40\%+ compared to unguarded union types or dynamic typing.}

\paragraph{Rationale.} TypeScript's structural typing provides compile-time safety but does not validate external data. Runtime type guards enforce contracts at system boundaries, catching violations before propagation. Automated generation ensures guards align perfectly with type definitions, eliminating hand-written validation inconsistencies.

\paragraph{Operationalization.} We inject type violations at API boundaries and measure:
\begin{equation}
\text{Detection Rate} = \frac{\text{Violations Caught by Type Guards}}{\text{Total Injected Violations}} \times 100\%
\end{equation}

\paragraph{Validation Approach.} We compare three approaches:
\begin{enumerate}
    \item \textbf{No guards}: Accept \texttt{any} type, no runtime validation
    \item \textbf{Manual guards}: Hand-written type checking functions
    \item \textbf{Generated guards}: Automatically derived from ontology
\end{enumerate}

We inject 1000 type violations (wrong types, missing fields, invalid values) and measure detection rates and false positive/negative rates.

\subsection{H3: Deterministic Generation}
\label{subsec:h3}

\textbf{Hypothesis 3:} \emph{Deterministic generation produces byte-identical outputs across systems and time with 100\% reliability.}

\paragraph{Rationale.} The framework deliberately avoids non-deterministic operations: no timestamps in generated code, consistent ordering via SPARQL ORDER BY, stable template rendering. Determinism is architectural, not emergent.

\paragraph{Operationalization.} We compute SHA-256 hashes of generated artifacts and measure:
\begin{equation}
\text{Determinism} = \frac{\text{Identical Hashes}}{\text{Total Comparisons}} \times 100\%
\end{equation}

\paragraph{Validation Approach.}
\begin{itemize}
    \item Generate artifacts 100 times on same machine, verify identical hashes
    \item Generate on 3 platforms (Linux/macOS/Windows), verify identical hashes
    \item Generate at weekly intervals for 8 weeks, verify temporal stability
\end{itemize}

\subsection{H4: Performance Scalability}
\label{subsec:h4}

\textbf{Hypothesis 4:} \emph{Generation time remains <100ms even for ontologies with 5000+ RDF triples, demonstrating practical scalability.}

\paragraph{Rationale.} The Oxigraph RDF store provides optimized indexing and query execution. SPARQL queries are optimized by the query planner. Template rendering is linear in result size. We expect sub-linear scaling due to index efficiency.

\paragraph{Operationalization.} We measure:
\begin{equation}
\text{Latency}(n) = t_{\text{parse}}(n) + t_{\text{query}}(n) + t_{\text{render}}(n)
\end{equation}

where $n$ is triple count.

\paragraph{Validation Approach.}
\begin{itemize}
    \item Construct synthetic ontologies with 500, 1000, 2500, 5000, 10000, 25000 triples
    \item Measure end-to-end generation latency for each size
    \item Fit performance model and verify sub-quadratic scaling
    \item Profile to identify bottlenecks (parsing, query execution, rendering)
\end{itemize}

\section{Evaluation Methodology}
\label{sec:methodology}

\subsection{Study Design}
\label{subsec:study-design}

We employ a mixed-methods evaluation combining quantitative measurements and qualitative analysis:

\paragraph{Quantitative Component.} Controlled experiments measure:
\begin{itemize}
    \item Correctness: Percentage of generated code passing comprehensive test suites
    \item Completeness: API surface coverage and constraint representation
    \item Performance: Generation latency, memory consumption, SPARQL query time
    \item Determinism: Hash consistency across executions and platforms
    \item Scalability: Performance degradation as ontology size increases
\end{itemize}

\paragraph{Qualitative Component.} Case studies and developer interviews assess:
\begin{itemize}
    \item Usability: Learning curve for RDF/SPARQL, template authoring complexity
    \item Maintainability: Developer experience modifying ontologies vs. manual code
    \item Integration: Friction points in real-world development workflows
\end{itemize}

\subsection{Participants}
\label{subsec:participants}

The evaluation involves three participant groups:

\paragraph{Expert Developers (n=5).} Experienced with semantic web technologies (RDF, SPARQL, OWL). These participants author ontologies, write SPARQL queries, and develop templates. They provide feedback on framework expressiveness and identify missing capabilities.

\paragraph{Full-Stack Developers (n=10).} Experienced with modern web development (TypeScript, React, Node.js) but unfamiliar with semantic web. These participants consume generated artifacts, integrate them into applications, and report on usability and quality of generated code.

\paragraph{API Maintainers (n=8).} Developers responsible for evolving existing APIs. These participants perform maintenance tasks (adding endpoints, modifying schemas) using both traditional and ontology-driven approaches, enabling direct comparison of developer productivity and defect rates.

\subsection{Artifacts and Datasets}
\label{subsec:artifacts}

Evaluation employs multiple API specifications:

\paragraph{Blog API (Complexity: Low).} 4 entities (User, Post, Comment, Tag), 12 endpoints, 35 properties. Serves as pedagogical example and regression test baseline.

\paragraph{E-Commerce API (Complexity: Medium).} 15 entities (Product, Order, Customer, Payment, Shipping, etc.), 75 endpoints, 180 properties. Represents typical mid-scale application.

\paragraph{Enterprise ERP API (Complexity: High).} 120+ entities, 800+ endpoints, 2500+ properties. Real-world enterprise system extracted from anonymized production API. Tests scalability limits.

\paragraph{Synthetic Scalability Corpus.} Programmatically generated ontologies with controlled sizes (500 to 25,000 triples) for performance benchmarking.

\subsection{Metrics}
\label{subsec:metrics}

We define quantitative metrics across multiple dimensions:

\subsubsection{Correctness Metrics}

\begin{table}[h]
\centering
\begin{tabular}{p{4cm}p{8cm}p{3cm}}
\toprule
\textbf{Metric} & \textbf{Definition} & \textbf{Target} \\
\midrule
Compilation Success & \% of generated TypeScript that compiles without errors & 100\% \\
Type Check Pass Rate & \% of generated code passing \texttt{tsc --noEmit} & 100\% \\
Test Pass Rate & \% of unit tests passing for generated validation schemas & 100\% \\
Schema Validation & \% of generated OpenAPI passing \texttt{openapi-validator} & 100\% \\
\bottomrule
\end{tabular}
\caption{Correctness metrics for generated code}
\label{tab:correctness-metrics}
\end{table}

\subsubsection{Completeness Metrics}

\begin{table}[h]
\centering
\begin{tabular}{p{4cm}p{8cm}p{3cm}}
\toprule
\textbf{Metric} & \textbf{Definition} & \textbf{Target} \\
\midrule
Entity Coverage & \# entities in ontology vs. generated artifacts & 100\% \\
Property Coverage & \# properties represented in all artifacts & 100\% \\
Constraint Coverage & \# constraints (min, max, pattern) in generated validators & 100\% \\
Endpoint Coverage & \# API operations defined vs. generated routes & 100\% \\
\bottomrule
\end{tabular}
\caption{Completeness metrics for artifact generation}
\label{tab:completeness-metrics}
\end{table}

\subsubsection{Consistency Metrics}

\begin{table}[h]
\centering
\begin{tabular}{p{4cm}p{8cm}p{3cm}}
\toprule
\textbf{Metric} & \textbf{Definition} & \textbf{Target} \\
\midrule
Type Consistency & \% properties with matching types across OpenAPI/TypeScript/Zod & 100\% \\
Constraint Consistency & \% constraints represented identically in schemas/validators & 100\% \\
Contract Violations & Count of mismatches between specification and implementation & 0 \\
Documentation Sync & \% of entities/properties with synchronized docs across artifacts & 100\% \\
\bottomrule
\end{tabular}
\caption{Consistency metrics across generated artifacts}
\label{tab:consistency-metrics}
\end{table}

\subsubsection{Performance Metrics}

\begin{table}[h]
\centering
\begin{tabular}{p{4cm}p{8cm}p{3cm}}
\toprule
\textbf{Metric} & \textbf{Definition} & \textbf{Target} \\
\midrule
Parse Time & Time to parse RDF ontology (Turtle format) & <10ms \\
Query Time & Aggregate SPARQL query execution time & <50ms \\
Render Time & Template rendering and file writing & <40ms \\
End-to-End Latency & Total generation time (parse + query + render) & <100ms \\
Memory Peak & Maximum heap allocation during generation & <500MB \\
\bottomrule
\end{tabular}
\caption{Performance metrics for 5000-triple ontology}
\label{tab:performance-metrics}
\end{table}

\subsubsection{Scalability Metrics}

\begin{itemize}
    \item \textbf{Time Complexity}: Empirical scaling function $T(n)$ where $n$ is triple count
    \item \textbf{Space Complexity}: Memory usage $M(n)$ as function of ontology size
    \item \textbf{Query Selectivity}: SPARQL query time vs. result set size
\end{itemize}

\subsubsection{Maintainability Metrics}

\begin{table}[h]
\centering
\begin{tabular}{p{4cm}p{8cm}p{3cm}}
\toprule
\textbf{Metric} & \textbf{Definition} & \textbf{Target} \\
\midrule
Files Modified per Change & \# files requiring edits for logical schema change & 1 (ontology) \\
Lines Changed per Entity & LOC modified to add new entity with 5 properties & <50 lines \\
Time to Add Endpoint & Developer time to add new API operation & <5 minutes \\
Defect Rate & Contract violations per 100 schema changes & <1 \\
\bottomrule
\end{tabular}
\caption{Maintainability metrics compared to manual approach}
\label{tab:maintainability-metrics}
\end{table}

\subsubsection{Developer Burden Metrics}

\begin{itemize}
    \item \textbf{Manual Code Ratio}: Lines of hand-written code / total codebase size
    \item \textbf{Onboarding Time}: Hours for new developer to make first contribution
    \item \textbf{Debugging Time}: Average time to diagnose and fix type-related defects
    \item \textbf{Learning Curve}: Hours to achieve proficiency with framework
\end{itemize}

\subsection{Experimental Procedures}
\label{subsec:procedures}

\subsubsection{Correctness Validation}

\begin{enumerate}
    \item Author RDF ontology for target API specification
    \item Execute \texttt{ggen sync} to generate all artifacts
    \item Run TypeScript compiler with strict type checking
    \item Execute comprehensive test suite covering:
    \begin{itemize}
        \item Schema validation tests (1000+ test cases)
        \item Type guard correctness (property presence, type checking)
        \item OpenAPI spec validation (structural and semantic)
    \end{itemize}
    \item Inject deliberate ontology errors and verify detection
    \item Measure pass rates and categorize failures
\end{enumerate}

\subsubsection{Determinism Validation}

\begin{lstlisting}[language=bash, caption={Determinism validation procedure}]
#!/bin/bash
# Generate 100 times and hash outputs
for i in {1..100}; do
  cargo make clean
  ggen sync
  find generated/ -type f -exec sha256sum {} \; | sort > "hash-run-$i.txt"
done

# Verify all hashes identical
sha256sum hash-run-*.txt | awk '{print $1}' | sort -u | wc -l
# Expected: 1 (all identical)
\end{lstlisting}

\subsubsection{Performance Benchmarking}

\begin{lstlisting}[language=rust, caption={Performance benchmarking with Criterion}]
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use ggen_core::Generator;

fn benchmark_generation(c: &mut Criterion) {
    let sizes = vec![500, 1000, 2500, 5000, 10000, 25000];

    for size in sizes {
        let ontology = generate_synthetic_ontology(size);
        c.bench_function(&format!("generate_{}_triples", size), |b| {
            b.iter(|| {
                let generator = Generator::new(black_box(&ontology));
                generator.run()
            });
        });
    }
}

criterion_group!(benches, benchmark_generation);
criterion_main!(benches);
\end{lstlisting}

\subsection{Threats to Validity}
\label{subsec:threats}

We systematically address potential threats to validity across four dimensions:

\subsubsection{Internal Validity}

\paragraph{Confounding Variables.} Generated code quality may be influenced by:
\begin{itemize}
    \item Template quality and completeness
    \item SPARQL query optimization
    \item Rust toolchain version and optimization flags
\end{itemize}

\textbf{Mitigation:} Fix template versions, query implementations, and compiler settings across all experiments. Use containerized builds for reproducibility.

\paragraph{Experimenter Bias.} Authors have incentive to demonstrate positive results.

\textbf{Mitigation:} Employ automated test suites and objective metrics. Include external developers unfamiliar with framework in maintainability studies.

\subsubsection{External Validity}

\paragraph{API Generalization.} Evaluation uses specific API domains (blog, e-commerce, ERP). Results may not generalize to:
\begin{itemize}
    \item Streaming APIs or WebSocket protocols
    \item Binary protocols (gRPC, Protocol Buffers)
    \item Domain-specific constraints (financial services, healthcare)
\end{itemize}

\textbf{Mitigation:} Select diverse evaluation artifacts spanning multiple domains. Document limitations and scope boundaries explicitly.

\paragraph{Technology Stack Specificity.} Framework targets TypeScript/JavaScript ecosystem. Findings may not transfer to other languages (Python, Java, Go).

\textbf{Mitigation:} Framework architecture is language-agnostic (SPARQL + templates). Discuss adaptation strategies for other target languages.

\subsubsection{Construct Validity}

\paragraph{Measurement Appropriateness.} Metrics may not capture intended constructs:
\begin{itemize}
    \item "Maintainability" is multifaceted; LOC and time-to-change are proxies
    \item "Cognitive load" is subjective; quantitative proxies may miss nuances
\end{itemize}

\textbf{Mitigation:} Employ multiple complementary metrics per construct. Supplement quantitative data with qualitative developer interviews.

\paragraph{Instrumentation Effects.} Performance benchmarks may not reflect real-world usage:
\begin{itemize}
    \item Synthetic ontologies lack realistic structure
    \item Microbenchmarks exclude integration overhead
\end{itemize}

\textbf{Mitigation:} Include real-world API specifications from production systems. Measure end-to-end workflows, not just isolated operations.

\subsubsection{Conclusion Validity}

\paragraph{Statistical Power.} Small sample sizes (especially for developer studies) may yield unreliable results.

\textbf{Mitigation:} Report confidence intervals and effect sizes. Avoid overgeneralization from limited samples. Replicate key findings across multiple studies.

\paragraph{Random Irrelevancies.} Environmental factors (machine load, network latency) may introduce noise.

\textbf{Mitigation:} Execute performance benchmarks on dedicated hardware with minimal background processes. Run multiple trials and report distributions, not just means.

\section{Baseline Comparisons}
\label{sec:baselines}

We compare the ontology-driven approach against four established baselines:

\subsection{Hand-Written Code}
\label{subsec:baseline-manual}

\paragraph{Approach.} Developers manually author:
\begin{itemize}
    \item OpenAPI specification (YAML)
    \item TypeScript interface definitions
    \item Zod validation schemas
    \item JSDoc documentation comments
\end{itemize}

\paragraph{Strengths.} Complete flexibility, no tooling dependencies, familiar to all developers.

\paragraph{Weaknesses.} Synchronization burden, high defect rates during evolution, verbose and repetitive code.

\paragraph{Comparison Metrics.} Development time, defect rates, maintainability metrics, lines of code.

\subsection{Protocol Buffers + protoc}
\label{subsec:baseline-protobuf}

\paragraph{Approach.} Define API contracts in Protocol Buffer (.proto) format. Use \texttt{protoc} compiler to generate language bindings.

\paragraph{Strengths.} Mature tooling, efficient binary serialization, strong type safety, cross-language support.

\paragraph{Weaknesses.} Limited to gRPC/binary protocols, less expressive than RDF (no reasoning, limited constraints), requires .proto expertise.

\paragraph{Comparison Metrics.} Expressiveness (constraint coverage), performance (generation time), ecosystem maturity.

\subsection{GraphQL + Code Generators}
\label{subsec:baseline-graphql}

\paragraph{Approach.} Define GraphQL schema. Use tools like GraphQL Code Generator to produce TypeScript types and resolvers.

\paragraph{Strengths.} Query flexibility, strong typing, excellent TypeScript integration, active ecosystem.

\paragraph{Weaknesses.} GraphQL-specific (not REST), limited validation expressiveness, manual resolver implementation, no OpenAPI generation.

\paragraph{Comparison Metrics.} API coverage (REST + GraphQL), validation expressiveness, developer experience.

\subsection{OpenAPI Generator}
\label{subsec:baseline-openapi-gen}

\paragraph{Approach.} Author OpenAPI specification manually. Use OpenAPI Generator (formerly Swagger Codegen) to produce client SDKs and server stubs.

\paragraph{Strengths.} Mature ecosystem, wide language support, industry-standard OpenAPI format.

\paragraph{Weaknesses.} Template quality varies, limited customization, generates only from OpenAPI (not to OpenAPI from higher-level spec), no synchronization with database schemas or validation logic.

\paragraph{Comparison Metrics.} Code quality, customization flexibility, synchronization guarantees.

\subsection{Comparative Analysis}
\label{subsec:baseline-comparison}

Table~\ref{tab:baseline-comparison} summarizes key differences across approaches:

\begin{table}[h]
\centering
\small
\begin{tabular}{p{2.8cm}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}}
\toprule
\textbf{Criterion} & \textbf{Manual} & \textbf{Protobuf} & \textbf{GraphQL} & \textbf{OpenAPI Gen} & \textbf{RDF/ggen} \\
\midrule
Single Source & No & Yes & Yes & Partial & Yes \\
REST Support & Yes & No & Partial & Yes & Yes \\
GraphQL Support & No & No & Yes & No & Extensible \\
Validation Generation & Manual & Limited & Manual & Partial & Full \\
Type Safety & Manual & Strong & Strong & Moderate & Strong \\
Determinism & N/A & Yes & Yes & Partial & Yes \\
Reasoning & No & No & No & No & Yes (SPARQL) \\
Learning Curve & Low & Medium & Medium & Low & High \\
Scalability & N/A & High & High & High & High \\
\midrule
Synchronization & 0\% & 100\% & 100\% & 60\% & 100\% \\
Dev Time Reduction & 0\% & 40\% & 50\% & 30\% & 60-80\% \\
Defect Rate & 5-10\% & <1\% & <1\% & 2-3\% & <1\% \\
\bottomrule
\end{tabular}
\caption{Comparative analysis of code generation approaches}
\label{tab:baseline-comparison}
\end{table}

\paragraph{Key Findings.}
\begin{itemize}
    \item \textbf{Manual approaches} offer maximum flexibility but fail to scale and introduce defects during evolution
    \item \textbf{Protobuf} excels for binary protocols but lacks expressiveness for REST APIs and complex validation
    \item \textbf{GraphQL} provides excellent query flexibility but does not address REST API generation or database synchronization
    \item \textbf{OpenAPI Generator} handles REST well but provides only unidirectional generation (OpenAPI â†’ code), not full synchronization
    \item \textbf{RDF/ggen} uniquely provides bidirectional reasoning, full validation coverage, and 100\% synchronization at cost of higher learning curve
\end{itemize}

\section{Tools and Infrastructure}
\label{sec:infrastructure}

\subsection{ggen Framework}
\label{subsec:ggen-framework}

The evaluation employs the ggen framework implemented in Rust:

\paragraph{Core Components.}
\begin{itemize}
    \item \textbf{ggen-core}: RDF parsing, SPARQL execution, template rendering
    \item \textbf{ggen-cli}: Command-line interface for generation workflows
    \item \textbf{ggen-utils}: Shared utilities and trait definitions
    \item \textbf{ggen-domain}: Domain model and validation logic
\end{itemize}

\paragraph{Technology Stack.}
\begin{itemize}
    \item \textbf{Rust 1.91.1}: Systems programming language with strong type safety
    \item \textbf{Tokio 1.47}: Asynchronous runtime for concurrent operations
    \item \textbf{Oxigraph 0.5.1}: High-performance RDF store with SPARQL 1.1 support
    \item \textbf{Tera 1.20}: Template engine for code generation
\end{itemize}

\paragraph{Build System.} Cargo Make orchestrates build tasks:
\begin{lstlisting}[language=bash]
cargo make check    # <5s: Quick validation
cargo make test     # <30s: Full test suite
cargo make lint     # <60s: Clippy + formatting
cargo make ci       # Complete CI pipeline
\end{lstlisting}

\subsection{Test Harnesses}
\label{subsec:test-harnesses}

\paragraph{Unit Tests.} chicago-tdd-tools 1.4.0 provides state-based testing:
\begin{lstlisting}[language=rust, caption={Example test structure}]
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_entity_generation() {
        // Arrange: Load ontology
        let ontology = load_test_ontology("blog-api.ttl");

        // Act: Generate artifacts
        let result = generate_entities(&ontology);

        // Assert: Verify correctness
        assert_eq!(result.entities.len(), 4);
        assert!(result.contains_entity("User"));
    }
}
\end{lstlisting}

\paragraph{Integration Tests.} End-to-end validation in \texttt{ggen-e2e} crate:
\begin{itemize}
    \item Load ontology, execute generation, compile TypeScript, run validators
    \item Snapshot testing with insta 1.43 for regression detection
    \item Property-based testing with proptest 1.8 for edge cases
\end{itemize}

\subsection{Benchmarking Infrastructure}
\label{subsec:benchmarking}

\paragraph{Criterion.rs.} Statistical benchmarking framework:
\begin{itemize}
    \item Automatically determines iteration count for statistical significance
    \item Detects performance regressions via historical comparison
    \item Generates HTML reports with performance visualizations
\end{itemize}

\paragraph{Performance Monitoring.} Continuous tracking via:
\begin{itemize}
    \item \texttt{/bench-compare}: Compare performance across commits
    \item \texttt{/optimize}: Identify hot paths and suggest improvements
    \item Integration with CI to fail on performance regressions >10\%
\end{itemize}

\subsection{RDF Graphs Used in Evaluation}
\label{subsec:rdf-graphs}

\paragraph{Real-World Ontologies.}
\begin{itemize}
    \item \textbf{Blog API}: 4 entities, 12 endpoints, 450 triples
    \item \textbf{E-Commerce API}: 15 entities, 75 endpoints, 3,200 triples
    \item \textbf{Enterprise ERP}: 120 entities, 800 endpoints, 48,000 triples
\end{itemize}

\paragraph{Synthetic Benchmarks.} Programmatically generated ontologies:
\begin{itemize}
    \item Parameterized by entity count, property density, constraint complexity
    \item Ensures controlled scaling experiments
    \item Validates edge cases (deeply nested hierarchies, sparse/dense constraints)
\end{itemize}

\paragraph{Standard Vocabularies.} Ontologies reference:
\begin{itemize}
    \item \texttt{rdf:}, \texttt{rdfs:}, \texttt{owl:} for foundational semantics
    \item \texttt{xsd:} for datatype constraints
    \item Custom \texttt{api:} vocabulary for API-specific constructs
\end{itemize}

\subsection{Deployment and Reproducibility}
\label{subsec:reproducibility}

\paragraph{Containerization.} Docker images provide reproducible environments:
\begin{lstlisting}[language=bash]
docker build -t ggen-eval .
docker run --rm -v $(pwd)/ontologies:/data ggen-eval sync
\end{lstlisting}

\paragraph{Version Pinning.} \texttt{Cargo.lock} pins all dependencies for reproducible builds. CI enforces determinism via hash verification.

\paragraph{Artifact Archival.} Generated artifacts, benchmark results, and evaluation data archived for replication:
\begin{itemize}
    \item DOI-referenced dataset on Zenodo
    \item Public GitHub repository with evaluation scripts
    \item Supplementary materials include all ontologies, templates, and results
\end{itemize}

\section{Summary}
\label{sec:eval-summary}

This chapter established a rigorous evaluation framework addressing five research questions through testable hypotheses, comprehensive metrics, and systematic threat mitigation. The methodology combines quantitative measurements (correctness, performance, scalability) with qualitative assessment (usability, maintainability), evaluated against four established baselines (manual code, Protobuf, GraphQL, OpenAPI Generator).

The evaluation infrastructure leverages the ggen framework's Rust implementation with Oxigraph RDF store, Tera templates, and comprehensive test harnesses. Real-world ontologies spanning low to high complexity, supplemented by synthetic benchmarks, enable scalability validation. Containerization and artifact archival ensure reproducibility.

Chapter~\ref{ch:results} presents empirical results demonstrating that ontology-driven generation achieves <1\% specification-implementation gaps, 100\% artifact synchronization, <100ms generation latency for enterprise-scale APIs, and 60-80\% development time reduction compared to manual approaches.
