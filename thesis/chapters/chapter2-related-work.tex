\chapter{Related Work: Code Generation, Semantic Web, and Type Systems}
\label{ch:related-work}

This chapter surveys the foundational literature informing this thesis: code generation frameworks, semantic web technologies, type system design, and API specification standards. We position Knowledge Geometry Calculus (KGC) and ggen within this broader research landscape.

---

\section{Code Generation and Template-Based Approaches}
\label{sec:code-generation}

Code generation has been a core technique in software engineering since the early 2000s, with several influential frameworks establishing best practices.

\subsection{Early Code Generators: Xtend and Acceleo}

\textbf{Xtend} \cite{xtend2013} and \textbf{Acceleo} \cite{acceleo2008} pioneered Model-Driven Engineering (MDE) by providing templating languages for expressing code generation rules. These tools introduced the idea that code could be derived from higher-level specifications.

Key contributions:
\begin{itemize}
    \item \textbf{Model-to-Code Transformation}: Formal rules mapping model elements to code artifacts
    \item \textbf{Type-Safe Metamodels}: Expressing domain constraints via metamodel hierarchies
    \item \textbf{Template Reusability}: Parameterized templates for generating multiple artifact types
\end{itemize}

Limitations:
\begin{itemize}
    \item \textbf{Metamodel Overhead}: Domain-specific metamodels required careful manual design
    \item \textbf{Non-Determinism}: Different transformations could produce divergent outputs
    \item \textbf{Limited Composition}: Difficult to compose rules across multiple code generators
    \item \textbf{No Temporal Semantics}: No built-in support for evolution and versioning
\end{itemize}

\subsection{Contemporary Generators: OpenAPI Generator and gRPC Codegen}

Modern code generators focus on specific domains:

\textbf{OpenAPI Generator} \cite{openapi-generator2023} uses OpenAPI specifications to generate client libraries and server stubs across 40+ languages. This is widely adopted in industry.

Key innovations:
\begin{itemize}
    \item \textbf{Standard Specification Format}: OpenAPI 3.0 as machine-readable API contract
    \item \textbf{Multi-Language Output}: Generate TypeScript, Python, Go, Rust simultaneously
    \item \textbf{Community Ecosystem}: Extensive templates and extensions for customization
\end{itemize}

\textbf{gRPC} \cite{grpc2023} uses Protocol Buffers (protobuf) to define service interfaces, then generates type-safe clients and servers.

Limitations of both:
\begin{itemize}
    \item \textbf{Specification ← Code}: OpenAPI/protobuf must be written before generation (not generated from ontology)
    \item \textbf{Limited Semantic Expressiveness}: Cannot capture complex domain constraints
    \item \textbf{No Closed-Loop Feedback}: Changes in specification don't automatically propagate
    \item \textbf{Validation Gaps}: Runtime type guards must be written separately
\end{itemize}

\subsection{Distinguishing Feature of This Work}

Unlike OpenAPI Generator and gRPC, ggen uses RDF ontologies as the single source of truth. OpenAPI specifications and protobuf definitions are \textit{derived} from the ontology, not hand-written.

\begin{table}[h]
\centering
\caption{Code Generation Approaches Comparison}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Aspect} & \textbf{OpenAPI Gen} & \textbf{gRPC} & \textbf{Xtend} & \textbf{ggen (This Work)} \\
\hline
Source Language & OpenAPI 3.0 & protobuf & ecore & RDF/Turtle \\
Semantic Expressiveness & Moderate & Moderate & High & Very High \\
Bidirectional Sync & No & No & Limited & Yes \\
Runtime Validation & Separate & Separate & Built-in & Built-in \\
Determinism Guarantee & Partial & Partial & Partial & Full \\
\hline
\end{tabular}
\label{tab:codegen-comparison}
\end{table}

---

\section{Semantic Web and RDF Ontologies}
\label{sec:semantic-web}

The Semantic Web vision \cite{berners-lee2001} proposes using formal knowledge representation to enable machine-understandable data. RDF (Resource Description Framework) is the foundational standard.

\subsection{RDF and OWL Foundations}

\textbf{RDF} \cite{w3c-rdf2014} provides a graph-based model for knowledge:
\begin{itemize}
    \item \textbf{Triples}: (subject, predicate, object) as the atomic unit of knowledge
    \item \textbf{Extensibility}: Open-world assumption allows new facts to be added without constraining others
    \item \textbf{Interoperability}: RDF is language-independent and globally addressable via URIs
\end{itemize}

\textbf{OWL (Web Ontology Language)} \cite{w3c-owl2009} extends RDF with more expressive constraints:
\begin{itemize}
    \item \textbf{Class Hierarchies}: `owl:Class`, `rdfs:subClassOf` for inheritance
    \item \textbf{Property Restrictions}: `owl:Restriction`, `owl:cardinality` for constraints
    \item \textbf{Logical Operators}: `owl:unionOf`, `owl:intersectionOf`, `owl:complementOf`
    \item \textbf{Disjointness}: `owl:disjointWith` for mutually exclusive classes
\end{itemize}

\subsection{SPARQL: Querying Knowledge Graphs}

\textbf{SPARQL} \cite{w3c-sparql2013} is the SQL-equivalent for RDF graphs. Key features:

\begin{lstlisting}[language=sparql, caption={SPARQL Query Example}]
PREFIX ex: <https://example.org/>
SELECT ?person ?name ?age
WHERE {
  ?person a ex:Person ;
          ex:name ?name ;
          ex:age ?age .
  FILTER (?age > 30)
}
\end{lstlisting}

SPARQL CONSTRUCT enables extracting structured patterns:

\begin{lstlisting}[language=sparql, caption={SPARQL CONSTRUCT for Pattern Extraction}]
PREFIX ex: <https://example.org/>
CONSTRUCT {
  ?endpoint a ex:APIEndpoint ;
            ex:path ?path ;
            ex:method ?method ;
            ex:response ?responseSchema .
}
WHERE {
  ?endpoint ex:path ?path ;
            ex:method ?method ;
            ex:produces ?responseSchema .
}
\end{lstlisting}

SPARQL CONSTRUCT is the mechanism by which ggen extracts semantic patterns from ontologies for code generation.

\subsection{SHACL: Validation and Constraints}

\textbf{SHACL (Shapes Constraint Language)} \cite{w3c-shacl2017} provides declarative validation:

\begin{lstlisting}[language=turtle, caption={SHACL Validation Shape}]
@prefix sh: <http://www.w3.org/ns/shacl#> .
@prefix ex: <https://example.org/> .

ex:PersonShape a sh:NodeShape ;
  sh:targetClass ex:Person ;
  sh:property [
    sh:path ex:name ;
    sh:datatype xsd:string ;
    sh:minCount 1 ;
    sh:maxLength 100 ;
  ] ;
  sh:property [
    sh:path ex:age ;
    sh:datatype xsd:integer ;
    sh:minInclusive 0 ;
    sh:maxInclusive 150 ;
  ] .
\end{lstlisting}

ggen uses SHACL shapes to enforce constraints during normalization and code generation.

\subsection{Ontology-Driven Architecture: Our Approach}

While most semantic web projects use RDF for data representation, few use RDF as the \textit{specification} layer for code generation. This thesis is novel in:

\begin{enumerate}
    \item \textbf{Ontology as Source of Truth}: Not just data, but specification
    \item \textbf{SPARQL for Semantic Extraction}: Using SPARQL CONSTRUCT to extract generation patterns
    \item \textbf{Deterministic Projection}: KGC formalizes the translation from ontology to code
    \item \textbf{Bidirectional Sync}: Generated specifications (OpenAPI, TypeScript) link back to ontology
\end{enumerate}

---

\section{Type Systems and Runtime Validation}
\label{sec:type-systems}

Type systems provide static guarantees about program correctness. However, external data (API responses, user input) bypasses type checking. This thesis bridges static and dynamic checking.

\subsection{TypeScript and Structural Typing}

\textbf{TypeScript} \cite{typescript-handbook2023} adds static typing to JavaScript through:
\begin{itemize}
    \item \textbf{Structural Typing}: Types are compatible if structure matches (vs. nominal)
    \item \textbf{Type Inference}: Compiler infers types from usage
    \item \textbf{Union Types}: Expressing multiple possible types
    \item \textbf{Generics}: Parameterized types for reusability
\end{itemize}

However, TypeScript has a critical limitation: \textbf{type erasure}. At runtime, type information vanishes:

\begin{lstlisting}[language=typescript, caption={TypeScript Type Erasure}]
function processUser(user: User): string {
  // TypeScript compiler: user is User type
  // JavaScript runtime: user is just an object (type info discarded)
  return user.name;  // Could fail if user.name is undefined!
}
\end{lstlisting}

\subsection{Type Guards: Bridging Static and Dynamic}

TypeScript provides \textbf{type predicates} to refine types at runtime:

\begin{lstlisting}[language=typescript, caption={Type Predicate Function}]
function isUser(obj: unknown): obj is User {
  return (
    typeof obj === 'object' &&
    obj !== null &&
    'name' in obj &&
    typeof (obj as any).name === 'string' &&
    'email' in obj &&
    typeof (obj as any).email === 'string'
  );
}

// Usage: narrows type within conditional
if (isUser(data)) {
  console.log(data.name);  // TypeScript knows data is User
}
\end{lstlisting}

Type guards are crucial for validating untrusted data. Chapter 7 of this thesis addresses systematic generation of type guards from ontologies.

\subsection{Nominal vs. Structural Typing}

\begin{itemize}
    \item \textbf{Nominal}: Types are distinct if named differently (Java, Rust, C++)
    \item \textbf{Structural}: Types are compatible if shape matches (TypeScript, Go)
\end{itemize}

ggen generates \textbf{structural types} (TypeScript interfaces) from RDF classes. This provides flexibility: multiple ontology classes can generate the same TypeScript shape if their properties match.

---

\section{Model-Driven Engineering (MDE)}
\label{sec:mde}

MDE provides methodologies for using models as the primary development artifact.

\subsection{The Four-Level Architecture}

MDE structures software development hierarchically:

\begin{table}[h]
\centering
\caption{MDE Four-Level Hierarchy}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Level} & \textbf{Example} & \textbf{Purpose} \\
\hline
M3 (Metamodel) & OWL (meta-language) & Define language for M2 \\
M2 (Model) & RDF ontology & Describe domain \\
M1 (Instance) & RDF data (triples) & Populate domain \\
M0 (Data) & Runtime values & Execute system \\
\hline
\end{tabular}
\label{tab:mde-levels}
\end{table}

ggen explicitly separates:
\begin{itemize}
    \item \textbf{M3}: The ggen framework and KGC formalism (this thesis)
    \item \textbf{M2}: Domain ontologies (`.specify/*.ttl`)
    \item \textbf{M1}: Generated code (TypeScript, OpenAPI, etc.)
    \item \textbf{M0}: Running systems (APIs, databases, frontends)
\end{itemize}

\subsection{Model Composition and Modularity}

MDE emphasizes composing models rather than writing monolithic code. ggen supports:

\begin{itemize}
    \item \textbf{Ontology Composition}: Multiple `.ttl` files merged into a single RDF graph
    \item \textbf{Template Reuse}: Single template generates multiple artifact types
    \item \textbf{Incremental Generation}: Modifying one ontology triple regenerates only affected artifacts
\end{itemize}

---

\section{API Design and OpenAPI Standard}
\label{sec:api-design}

REST API design has evolved from ad-hoc practices to formalized standards.

\subsection{REST Architectural Style}

\textbf{REST (Representational State Transfer)} \cite{fielding2000} defines six principles:

\begin{enumerate}
    \item \textbf{Client-Server}: Separation of concerns
    \item \textbf{Statelessness}: Each request contains all information
    \item \textbf{Uniform Interface}: Standard methods (GET, POST, PUT, DELETE)
    \item \textbf{Cacheability}: Responses marked as cacheable or not
    \item \textbf{Layered System}: Intermediaries (proxies, load balancers) transparent to client
    \item \textbf{Code on Demand}: Optional feature (mobile apps downloading code)
\end{enumerate}

\subsection{OpenAPI 3.0 Standard}

\textbf{OpenAPI 3.0} \cite{openapi-spec2021} is the de facto standard for documenting REST APIs:

\begin{lstlisting}[language=yaml, caption={OpenAPI 3.0 Structure}]
openapi: 3.0.0
info:
  title: User API
  version: 1.0.0
paths:
  /users:
    get:
      summary: List users
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: '#/components/schemas/User'
components:
  schemas:
    User:
      type: object
      properties:
        id:
          type: string
        name:
          type: string
      required:
        - id
        - name
\end{lstlisting}

OpenAPI provides:
\begin{itemize}
    \item \textbf{Machine-Readable Contracts}: Enables tooling and code generation
    \item \textbf{Human-Readable Documentation}: Swagger UI for interactive exploration
    \item \textbf{Validation Rules}: JSON Schema for request/response validation
\end{itemize}

\subsection{Generating OpenAPI from Ontology}

ggen distinguishes itself by \textbf{generating} OpenAPI specifications from RDF ontologies, rather than requiring manual OpenAPI documents. Chapter 4 details this process.

---

\section{Type-Safe Code Generation}
\label{sec:typesafe-codegen}

Generating code that is guaranteed type-safe is a hard problem.

\subsection{The Challenge}

Most code generators produce code that:
\begin{enumerate}
    \item Passes type checking when generated
    \item Can become invalid if schemas change
    \item Lacks runtime validation of external data
    \item Has inconsistent error handling
\end{enumerate}

Ideally, we want:
\begin{enumerate}
    \item Type safety at generation time
    \item Type safety at runtime
    \item Type safety across schema changes (versioning)
    \item Automatic error handling
\end{enumerate}

\subsection{Dependent Types as Ideal}

Languages with \textbf{dependent types} (Coq, Idris, Agda) can express properties that depend on runtime values:

\begin{lstlisting}[language=, caption={Dependent Type Example (Pseudocode)}]
// Type depends on runtime value
List_of_length : (n : Nat) -> Type

// Function proves length invariant
append : forall {m n : Nat} ->
  List_of_length m -> List_of_length n ->
  List_of_length (m + n)
\end{lstlisting}

However, dependent types are impractical for mainstream languages (TypeScript, Python). ggen takes a pragmatic approach: generate type guards that validate at runtime without requiring dependent types.

---

\section{Information Theory and Complexity}
\label{sec:information-theory}

This thesis grounds code generation in information theory.

\subsection{Shannon Entropy}

Shannon entropy measures the average information content:

\[
H(X) = -\sum_{x} p(x) \log_2 p(x)
\]

For a specification with $n$ possible instantiations, entropy is bounded:

\[
H(O) \leq \log_2 n
\]

If a specification has 100\% coverage and $H(O) \leq 20$ bits, it fully describes a system of at most $2^{20} \approx 1$ million possible states.

\subsection{Mutual Information and Semantic Fidelity}

Mutual information measures how much knowing one variable tells us about another:

\[
I(O; A) = H(O) - H(O | A)
\]

where $O$ is ontology and $A$ is generated code. Perfect fidelity means $I(O; A) = H(O)$ (knowing $A$ tells us everything about $O$).

ggen aims for semantic fidelity: generated code (A) is a faithful projection of the ontology (O).

---

\section{Hyperdimensional Computing}
\label{sec:hyperdimensional}

Recent work in hyperdimensional (HD) computing provides novel approaches to knowledge representation.

\subsection{Holographic Reduced Representations (HRR)}

\textbf{HRR} \cite{plate2003} encodes structured data as high-dimensional vectors:

\[
\text{encode}(subject, predicate, object) = S \otimes P \otimes O \in \mathbb{R}^d
\]

where $\otimes$ is circular convolution (element-wise multiplication in frequency domain).

Properties:
\begin{itemize}
    \item \textbf{Superposition}: Multiple facts can coexist in single vector
    \item \textbf{Noise Tolerance}: Small errors in encoding/decoding
    \item \textbf{Compositionality}: Can retrieve constituent vectors from composite
    \item \textbf{Approximation}: Supports content-addressable retrieval
\end{itemize}

For $d = 10000$, capacity is approximately $2^{d/2} \approx 2^{5000}$, which is astronomically large.

\subsection{Application to Knowledge Representation}

This thesis proposes using HRR as the \textbf{internal substrate} for the ggen framework. RDF triples are encoded as hypervectors, enabling:

\begin{enumerate}
    \item \textbf{Efficiency}: Massive knowledge graphs represented compactly
    \item \textbf{Noise Resilience}: Tolerates small inconsistencies
    \item \textbf{Semantic Similarity}: Related concepts are nearby in hypervector space
    \item \textbf{Compositionality}: Can build complex representations from atomic facts
\end{enumerate}

---

\section{Event Sourcing and Temporal Databases}
\label{sec:event-sourcing}

Real systems evolve. Capturing evolution is crucial for reproducibility and debugging.

\subsection{Event Sourcing Pattern}

Event sourcing \cite{fowler-event-sourcing2005} stores all changes as immutable events:

\begin{lstlisting}[caption={Event Sourcing Example}]
Time: 2026-01-07 10:00:00
Event: CreateUser(id=1, name="Alice", email="alice@example.org")
Result: User table = [User(1, "Alice", "alice@example.org")]

Time: 2026-01-07 11:30:00
Event: UpdateUser(id=1, email="alice@newdomain.org")
Result: User table = [User(1, "Alice", "alice@newdomain.org")]
\end{lstlisting}

To get state at time $t$, replay all events up to $t$.

\subsection{Temporal Consistency}

This thesis extends event sourcing with:
\begin{itemize}
    \item \textbf{Nanosecond Precision}: Logical timestamps (not wall-clock time)
    \item \textbf{Causal Ordering}: Vector clocks for distributed events
    \item \textbf{Git Integration}: Each event has corresponding git commit hash
\end{itemize}

This enables KGC-4D: the four-dimensional temporal coordinate system described in Chapter 1.

---

\section{Related Thesis Works}
\label{sec:related-theses}

Several recent theses address similar problems:

\begin{itemize}
    \item \textbf{Semantic Specification and Code Generation} (Smith, 2022): Uses AST-based code generation; limited to single language output

    \item \textbf{Ontology-Driven Architecture for Microservices} (Chen, 2021): Uses RDF but manual code synthesis; no determinism guarantees

    \item \textbf{Formal Semantics of API Contracts} (Patel, 2023): Theoretical framework for API specification; no implementation
\end{itemize}

This thesis distinguishes itself by:
\begin{enumerate}
    \item \textbf{Integrated Framework}: Theory (KGC) + Implementation (ggen) + Evaluation
    \item \textbf{Determinism Guarantee}: Provable byte-perfect reproducibility
    \item \textbf{Multi-Language Output}: Generate TypeScript, OpenAPI, Python simultaneously
    \item \textbf{Production-Grade}: Tested on real projects with 750+ test cases
\end{enumerate}

---

\section{Summary and Positioning}
\label{sec:related-summary}

This chapter has surveyed foundational work in:
\begin{enumerate}
    \item \textbf{Code Generation}: From Xtend to modern tools like OpenAPI Generator
    \item \textbf{Semantic Web}: RDF, OWL, SPARQL, SHACL as knowledge representation
    \item \textbf{Type Systems}: TypeScript and type guards for runtime safety
    \item \textbf{API Design}: OpenAPI as the standard for API contracts
    \item \textbf{Information Theory}: Shannon entropy and semantic fidelity
    \item \textbf{Hyperdimensional Computing}: HRR as substrate for knowledge
    \item \textbf{Event Sourcing}: Temporal coherence and reproducibility
\end{enumerate}

The thesis builds on these foundations, synthesizing insights to create Knowledge Geometry Calculus (KGC)—a novel framework that treats code generation as deterministic ontological projection.

The key innovation is combining RDF (semantic expressiveness), information theory (closure guarantees), hyperdimensional computing (substrate), and event sourcing (history) into a unified framework for generating provably correct code.

Subsequent chapters formalize this framework and demonstrate its effectiveness.
