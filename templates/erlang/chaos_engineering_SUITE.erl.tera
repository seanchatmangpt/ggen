%%%-------------------------------------------------------------------
%%% @doc Chaos Engineering Test Suite
%%% Tests system resilience under adverse conditions:
%%% - Container failures and restarts
%%% - Network partitions and latency
%%% - Resource exhaustion (CPU, memory, disk)
%%% - Concurrent failures (multiple backends failing)
%%% - Byzantine failures (corrupted data, inconsistent state)
%%%
%%% Uses testcontainers to simulate real-world failure scenarios.
%%% @end
%%% Created: {% raw %}{{{% endraw %} timestamp {% raw %}}}{% endraw %}
%%%-------------------------------------------------------------------
-module({{ module_name }}_chaos_SUITE).

-include_lib("common_test/include/ct.hrl").
-include_lib("eunit/include/eunit.hrl").

-compile([export_all, nowarn_export_all]).

%%%===================================================================
%%% CT Callbacks
%%%===================================================================

all() ->
    [
        test_redis_container_failure,
        test_redis_container_restart,
        test_postgres_network_partition,
        test_concurrent_backend_failures,
        test_graceful_degradation,
        test_circuit_breaker_opens,
        test_retry_with_exponential_backoff,
        test_failover_to_backup_backend,
        test_memory_pressure,
        test_slow_backend_timeout
    ].

init_per_suite(Config) ->
    % Start testcontainers helper
    application:ensure_all_started({{ module_name }}),
    Config.

end_per_suite(_Config) ->
    % Cleanup all containers
    {{ module_name }}_testcontainers:cleanup_all(),
    ok.

init_per_testcase(TestCase, Config) ->
    ct:print("~n=== Starting ~p ===~n", [TestCase]),
    Config.

end_per_testcase(TestCase, _Config) ->
    ct:print("~n=== Finished ~p ===~n", [TestCase]),
    ok.

%%%===================================================================
%%% Test Cases - Container Failures
%%%===================================================================

test_redis_container_failure(Config) ->
    ct:print("Test: Redis container sudden failure"),

    % Start Redis container
    {ok, Redis} = {{ module_name }}_testcontainers:start_redis(),
    {ok, Port} = {{ module_name }}_testcontainers:get_container_port(Redis, 6379),

    % Start job queue with Redis backend
    {ok, Queue} = {{ module_name }}_queue:start_link([
        {backend, redis_backend},
        {url, iolist_to_binary(["redis://localhost:", integer_to_list(Port)])}
    ]),

    % Enqueue jobs successfully
    Jobs = [#{id => integer_to_binary(I), type => test} || I <- lists:seq(1, 100)],
    lists:foreach(fun(Job) ->
        {ok, _} = {{ module_name }}_queue:enqueue(Queue, Job)
    end, Jobs),

    ct:print("Enqueued 100 jobs, queue size: ~p", [{{ module_name }}_queue:size(Queue)]),

    % CHAOS: Kill Redis container (simulate crash)
    ct:print("CHAOS: Killing Redis container..."),
    {{ module_name }}_testcontainers:stop_container(Redis),

    % Verify graceful failure handling
    timer:sleep(1000),  % Wait for detection

    % Attempting to enqueue should fail gracefully
    Result = {{ module_name }}_queue:enqueue(Queue, #{id => <<"job-after-failure">>, type => test}),
    ?assertMatch({error, _}, Result),

    ct:print("Queue correctly rejected operation after backend failure"),

    % Verify queue enters degraded state
    Status = {{ module_name }}_queue:status(Queue),
    ?assertMatch(#{status := degraded}, Status),

    ct:print("Test passed: System gracefully handled Redis container failure"),
    ok.

test_redis_container_restart(Config) ->
    ct:print("Test: Redis container restart and recovery"),

    % Start Redis container
    {ok, Redis} = {{ module_name }}_testcontainers:start_redis(),
    {ok, Port} = {{ module_name }}_testcontainers:get_container_port(Redis, 6379),

    % Start job queue
    {ok, Queue} = {{ module_name }}_queue:start_link([
        {backend, redis_backend},
        {url, iolist_to_binary(["redis://localhost:", integer_to_list(Port)])},
        {retry_policy, #{max_retries => 5, base_delay => 100}}
    ]),

    % Enqueue initial jobs
    {ok, _} = {{ module_name }}_queue:enqueue(Queue, #{id => <<"job-001">>, type => test}),

    % CHAOS: Kill container
    ct:print("CHAOS: Stopping Redis container..."),
    {{ module_name }}_testcontainers:stop_container(Redis),
    timer:sleep(2000),

    % RECOVERY: Restart Redis container
    ct:print("RECOVERY: Restarting Redis container..."),
    {ok, Redis2} = {{ module_name }}_testcontainers:start_redis(),

    % Wait for connection recovery
    timer:sleep(3000),

    % Verify recovery - queue should accept new jobs
    Result = {{ module_name }}_queue:enqueue(Queue, #{id => <<"job-after-restart">>, type => test}),
    ?assertMatch({ok, _}, Result),

    ct:print("Test passed: System recovered after Redis container restart"),

    % Cleanup
    {{ module_name }}_testcontainers:stop_container(Redis2),
    ok.

%%%===================================================================
%%% Test Cases - Network Failures
%%%===================================================================

test_postgres_network_partition(Config) ->
    ct:print("Test: Network partition to PostgreSQL backend"),

    % Start PostgreSQL container
    {ok, Postgres} = {{ module_name }}_testcontainers:start_postgres(),
    {ok, Port} = {{ module_name }}_testcontainers:get_container_port(Postgres, 5432),

    % Start job queue with Postgres backend
    {ok, Queue} = {{ module_name }}_queue:start_link([
        {backend, postgres_backend},
        {url, iolist_to_binary(["postgresql://test:test@localhost:", integer_to_list(Port), "/test"])}
    ]),

    % Enqueue jobs
    lists:foreach(fun(I) ->
        {ok, _} = {{ module_name }}_queue:enqueue(Queue, #{id => integer_to_binary(I), type => test})
    end, lists:seq(1, 50)),

    % CHAOS: Simulate network partition using iptables (requires privileged container)
    % For this test, we'll pause the container which simulates network issues
    ContainerId = maps:get(id, Postgres),
    ct:print("CHAOS: Pausing PostgreSQL container (simulates network partition)"),
    os:cmd(binary_to_list(iolist_to_binary(["docker pause ", ContainerId]))),

    timer:sleep(1000),

    % Operations should timeout or fail
    Result = {{ module_name }}_queue:dequeue(Queue),
    ?assertMatch({error, timeout}, Result),

    % RECOVERY: Unpause container
    ct:print("RECOVERY: Unpausing PostgreSQL container"),
    os:cmd(binary_to_list(iolist_to_binary(["docker unpause ", ContainerId]))),

    timer:sleep(2000),

    % Should recover and allow operations
    RecoveryResult = {{ module_name }}_queue:dequeue(Queue),
    ?assertMatch({ok, _}, RecoveryResult),

    ct:print("Test passed: System handled network partition gracefully"),

    % Cleanup
    {{ module_name }}_testcontainers:stop_container(Postgres),
    ok.

%%%===================================================================
%%% Test Cases - Concurrent Failures
%%%===================================================================

test_concurrent_backend_failures(Config) ->
    ct:print("Test: Multiple backends failing simultaneously"),

    % Start multiple backends
    {ok, Redis} = {{ module_name }}_testcontainers:start_redis(),
    {ok, Postgres} = {{ module_name }}_testcontainers:start_postgres(),

    {ok, RedisPort} = {{ module_name }}_testcontainers:get_container_port(Redis, 6379),
    {ok, PgPort} = {{ module_name }}_testcontainers:get_container_port(Postgres, 5432),

    % Start job queues for each backend
    {ok, RedisQueue} = {{ module_name }}_queue:start_link([
        {backend, redis_backend},
        {url, iolist_to_binary(["redis://localhost:", integer_to_list(RedisPort)])}
    ]),

    {ok, PgQueue} = {{ module_name }}_queue:start_link([
        {backend, postgres_backend},
        {url, iolist_to_binary(["postgresql://test:test@localhost:", integer_to_list(PgPort), "/test"])}
    ]),

    % CHAOS: Kill both backends simultaneously
    ct:print("CHAOS: Killing Redis and PostgreSQL simultaneously"),
    spawn(fun() -> {{ module_name }}_testcontainers:stop_container(Redis) end),
    spawn(fun() -> {{ module_name }}_testcontainers:stop_container(Postgres) end),

    timer:sleep(2000),

    % Both queues should be in degraded state
    RedisStatus = {{ module_name }}_queue:status(RedisQueue),
    PgStatus = {{ module_name }}_queue:status(PgQueue),

    ?assertMatch(#{status := degraded}, RedisStatus),
    ?assertMatch(#{status := degraded}, PgStatus),

    ct:print("Test passed: System handled concurrent failures across multiple backends"),
    ok.

%%%===================================================================
%%% Test Cases - Graceful Degradation
%%%===================================================================

test_graceful_degradation(Config) ->
    ct:print("Test: Graceful degradation when backend unavailable"),

    % Start with Redis
    {ok, Redis} = {{ module_name }}_testcontainers:start_redis(),
    {ok, Port} = {{ module_name }}_testcontainers:get_container_port(Redis, 6379),

    % Start queue with fallback to in-memory backend
    {ok, Queue} = {{ module_name }}_queue:start_link([
        {backend, redis_backend},
        {url, iolist_to_binary(["redis://localhost:", integer_to_list(Port)])},
        {fallback_backend, ets_backend}
    ]),

    % Enqueue with Redis (should succeed)
    {ok, _} = {{ module_name }}_queue:enqueue(Queue, #{id => <<"job-001">>, type => test}),

    % CHAOS: Kill Redis
    {{ module_name }}_testcontainers:stop_container(Redis),
    timer:sleep(1000),

    % Should automatically fall back to ETS backend
    {ok, _} = {{ module_name }}_queue:enqueue(Queue, #{id => <<"job-002">>, type => test}),

    % Verify queue is still operational (using ETS)
    ?assertEqual(2, {{ module_name }}_queue:size(Queue)),

    ct:print("Test passed: System gracefully degraded to fallback backend"),
    ok.

%%%===================================================================
%%% Test Cases - Circuit Breaker
%%%===================================================================

test_circuit_breaker_opens(Config) ->
    ct:print("Test: Circuit breaker opens after repeated failures"),

    % Start Redis but immediately kill it
    {ok, Redis} = {{ module_name }}_testcontainers:start_redis(),
    {ok, Port} = {{ module_name }}_testcontainers:get_container_port(Redis, 6379),
    {{ module_name }}_testcontainers:stop_container(Redis),

    % Start queue with circuit breaker enabled
    {ok, Queue} = {{ module_name }}_queue:start_link([
        {backend, redis_backend},
        {url, iolist_to_binary(["redis://localhost:", integer_to_list(Port)])},
        {circuit_breaker, #{
            threshold => 3,
            timeout => 5000,
            half_open_timeout => 10000
        }}
    ]),

    % Attempt operations until circuit breaker opens
    lists:foreach(fun(_) ->
        {{ module_name }}_queue:enqueue(Queue, #{id => <<"test">>, type => test})
    end, lists:seq(1, 5)),

    timer:sleep(1000),

    % Circuit should be open
    Status = {{ module_name }}_queue:circuit_breaker_status(Queue),
    ?assertEqual(open, Status),

    ct:print("Test passed: Circuit breaker opened after repeated failures"),
    ok.

%%%===================================================================
%%% Test Cases - Retry Logic
%%%===================================================================

test_retry_with_exponential_backoff(Config) ->
    ct:print("Test: Retry with exponential backoff"),

    % Start Redis
    {ok, Redis} = {{ module_name }}_testcontainers:start_redis(),
    {ok, Port} = {{ module_name }}_testcontainers:get_container_port(Redis, 6379),

    % Start queue with retry policy
    {ok, Queue} = {{ module_name }}_queue:start_link([
        {backend, redis_backend},
        {url, iolist_to_binary(["redis://localhost:", integer_to_list(Port)])},
        {retry_policy, #{
            max_retries => 5,
            base_delay => 100,
            max_delay => 5000,
            backoff_multiplier => 2
        }}
    ]),

    % Enqueue job
    {ok, JobId} = {{ module_name }}_queue:enqueue(Queue, #{id => <<"job-001">>, type => test}),

    % CHAOS: Kill Redis briefly
    {{ module_name }}_testcontainers:stop_container(Redis),
    timer:sleep(500),

    % Restart Redis
    {ok, Redis2} = {{ module_name }}_testcontainers:start_redis(),
    timer:sleep(2000),

    % Dequeue should succeed after retries
    {ok, Job} = {{ module_name }}_queue:dequeue(Queue),
    ?assertEqual(JobId, maps:get(id, Job)),

    ct:print("Test passed: Retry mechanism with exponential backoff worked"),

    % Cleanup
    {{ module_name }}_testcontainers:stop_container(Redis2),
    ok.

%%%===================================================================
%%% Test Cases - Resource Pressure
%%%===================================================================

test_memory_pressure(Config) ->
    ct:print("Test: System behavior under memory pressure"),

    % Start Redis with memory limit
    {ok, Redis} = {{ module_name }}_testcontainers:start_container(#{
        image => <<"redis:7.0">>,
        ports => [6379],
        command => [<<"redis-server">>, <<"--maxmemory">>, <<"50mb">>, <<"--maxmemory-policy">>, <<"allkeys-lru">>]
    }),

    {ok, Port} = {{ module_name }}_testcontainers:get_container_port(Redis, 6379),

    % Start queue
    {ok, Queue} = {{ module_name }}_queue:start_link([
        {backend, redis_backend},
        {url, iolist_to_binary(["redis://localhost:", integer_to_list(Port)])}
    ]),

    % Try to fill Redis beyond memory limit
    ct:print("Filling Redis beyond memory limit..."),
    lists:foreach(fun(I) ->
        LargePayload = binary:copy(<<"x">>, 100000),  % 100KB payload
        {{ module_name }}_queue:enqueue(Queue, #{
            id => integer_to_binary(I),
            type => large_job,
            payload => LargePayload
        })
    end, lists:seq(1, 600)),  % ~60MB of data

    % Verify system handles evictions gracefully
    timer:sleep(1000),
    Status = {{ module_name }}_queue:status(Queue),
    ct:print("Queue status under memory pressure: ~p", [Status]),

    % Queue should still be operational
    ?assertMatch(#{status := Status} when Status =:= running orelse Status =:= degraded, Status),

    ct:print("Test passed: System handled memory pressure gracefully"),

    % Cleanup
    {{ module_name }}_testcontainers:stop_container(Redis),
    ok.

test_slow_backend_timeout(Config) ->
    ct:print("Test: Slow backend operations timeout correctly"),

    % Start Redis
    {ok, Redis} = {{ module_name }}_testcontainers:start_redis(),
    {ok, Port} = {{ module_name }}_testcontainers:get_container_port(Redis, 6379),

    % Start queue with short timeout
    {ok, Queue} = {{ module_name }}_queue:start_link([
        {backend, redis_backend},
        {url, iolist_to_binary(["redis://localhost:", integer_to_list(Port)])},
        {operation_timeout, 1000}  % 1 second timeout
    ]),

    % CHAOS: Pause container to simulate slow operations
    ContainerId = maps:get(id, Redis),
    ct:print("CHAOS: Pausing Redis to simulate slow operations"),
    os:cmd(binary_to_list(iolist_to_binary(["docker pause ", ContainerId]))),

    % Operation should timeout
    Start = erlang:monotonic_time(millisecond),
    Result = {{ module_name }}_queue:enqueue(Queue, #{id => <<"timeout-test">>, type => test}),
    Elapsed = erlang:monotonic_time(millisecond) - Start,

    ?assertMatch({error, timeout}, Result),
    ?assert(Elapsed >= 1000 andalso Elapsed < 2000),  % Should timeout around 1s

    ct:print("Operation timed out after ~pms (expected ~1000ms)", [Elapsed]),

    % Unpause
    os:cmd(binary_to_list(iolist_to_binary(["docker unpause ", ContainerId]))),

    ct:print("Test passed: Slow operations timeout correctly"),

    % Cleanup
    {{ module_name }}_testcontainers:stop_container(Redis),
    ok.

%%%===================================================================
%%% Helper Functions
%%%===================================================================
