//! London TDD tests for `ggen ai generate` command
//!
//! README.md Â§AI-Powered Generation - Template Generation
//!
//! Tests verify:
//! - AI template generation from natural language
//! - Multi-provider support (OpenAI, Anthropic, Ollama)
//! - YAML frontmatter generation
//! - Tera template body generation

use crate::lib::*;
use mockall::predicate::*;

#[test]
fn test_ai_generate_creates_template_from_description() {
    let start = std::time::Instant::now();

    // Arrange
    let mut mock_llm = MockLlmClient::new();
    let mut mock_fs = MockFilesystem::new();

    mock_llm
        .expect_generate()
        .with(
            predicate::str::contains("REST API module"),
            eq("gpt-4o"),
        )
        .times(1)
        .returning(|_, _| {
            Ok(r#"---
to: "src/api/{{name}}.rs"
vars:
  name: "example"
---
pub struct {{name | capitalize}}Api {
    // API implementation
}
"#.to_string())
        });

    mock_fs
        .expect_write_file()
        .times(1)
        .returning(|_, _| Ok(()));

    // Act
    let result = run_ai_generate_command(
        &mock_llm,
        &mock_fs,
        "REST API module",
        "gpt-4o",
        "api_module.tmpl",
    );

    // Assert
    assert!(result.is_ok());
    let template = result.unwrap();
    assert!(template.frontmatter.contains("to:"));
    assert!(template.body.contains("pub struct"));

    // Performance
    assert!(start.elapsed().as_millis() < 100);
}

#[test]
fn test_ai_generate_supports_multiple_providers() {
    // Test OpenAI
    let mut mock_openai = MockLlmClient::new();
    mock_openai
        .expect_generate()
        .with(anything(), eq("gpt-4o"))
        .returning(|_, _| Ok("---\nto: test\n---\ntemplate".to_string()));

    let result = run_ai_generate_with_provider(&mock_openai, "openai", "gpt-4o");
    assert!(result.is_ok());

    // Test Anthropic
    let mut mock_anthropic = MockLlmClient::new();
    mock_anthropic
        .expect_generate()
        .with(anything(), eq("claude-3-5-sonnet-20241022"))
        .returning(|_, _| Ok("---\nto: test\n---\ntemplate".to_string()));

    let result = run_ai_generate_with_provider(&mock_anthropic, "anthropic", "claude-3-5-sonnet-20241022");
    assert!(result.is_ok());

    // Test Ollama
    let mut mock_ollama = MockLlmClient::new();
    mock_ollama
        .expect_generate()
        .with(anything(), eq("qwen3-coder:30b"))
        .returning(|_, _| Ok("---\nto: test\n---\ntemplate".to_string()));

    let result = run_ai_generate_with_provider(&mock_ollama, "ollama", "qwen3-coder:30b");
    assert!(result.is_ok());
}

#[test]
fn test_ai_generate_validates_frontmatter_syntax() {
    // Arrange
    let mut mock_llm = MockLlmClient::new();
    let mock_fs = MockFilesystem::new();

    // LLM returns invalid YAML
    mock_llm
        .expect_generate()
        .returning(|_, _| Ok("---\ninvalid yaml: [unclosed\n---\nbody".to_string()));

    // Act
    let result = run_ai_generate_command(&mock_llm, &mock_fs, "test", "gpt-4o", "out.tmpl");

    // Assert: Validation error
    assert!(result.is_err());
    let err = result.unwrap_err();
    assert!(err.to_string().contains("Invalid YAML") || err.to_string().contains("frontmatter"));
}

#[test]
fn test_ai_generate_includes_reasonable_defaults() {
    // Arrange
    let mut mock_llm = MockLlmClient::new();
    let mock_fs = MockFilesystem::new();

    mock_llm.expect_generate().returning(|_, _| {
        Ok(r#"---
to: "output.rs"
vars:
  author: "ggen"
  timestamp: "2025-01-01"
---
// Generated by {{author}} on {{timestamp}}
"#
        .to_string())
    });

    // Act
    let result = run_ai_generate_command(&mock_llm, &mock_fs, "simple template", "gpt-4o", "out.tmpl");

    // Assert: Contains standard metadata
    assert!(result.is_ok());
    let template = result.unwrap();
    assert!(template.frontmatter.contains("author"));
    assert!(template.frontmatter.contains("ggen"));
}

#[test]
fn test_ai_generate_creates_otel_span() {
    // Arrange
    let mut mock_llm = MockLlmClient::new();
    let mock_fs = MockFilesystem::new();
    let tracer = otel::MockTracerProvider::new();

    mock_llm
        .expect_generate()
        .returning(|_, _| Ok("---\nto: test\n---\nbody".to_string()));

    // Act
    let _result = run_ai_generate_with_tracing(&mock_llm, &mock_fs, &tracer, "test description");

    // Assert
    let span = tracer.find_span("ggen.ai.generate").unwrap();
    assert_eq!(span.status, otel::SpanStatus::Ok);
    assert!(span.attributes.iter().any(|(k, _)| k == "ai.provider"));
    assert!(span.attributes.iter().any(|(k, _)| k == "ai.model"));
}

// Helper types and functions

#[derive(Debug)]
struct GeneratedTemplate {
    frontmatter: String,
    body: String,
}

fn run_ai_generate_command(
    llm: &dyn LlmClient,
    fs: &dyn Filesystem,
    description: &str,
    model: &str,
    output_path: &str,
) -> Result<GeneratedTemplate, anyhow::Error> {
    let prompt = format!(
        "Generate a ggen template for: {}\n\nInclude YAML frontmatter with 'to:', 'vars:', and a Tera template body.",
        description
    );

    let response = llm.generate(&prompt, model)?;

    // Parse frontmatter and body
    let parts: Vec<&str> = response.split("---").collect();
    if parts.len() < 3 {
        return Err(anyhow::anyhow!("Invalid template format"));
    }

    let frontmatter = parts[1].trim();
    let body = parts[2].trim();

    // Validate YAML
    serde_yaml::from_str::<serde_yaml::Value>(frontmatter)
        .map_err(|_| anyhow::anyhow!("Invalid YAML frontmatter"))?;

    // Write to file
    fs.write_file(output_path, &response)?;

    Ok(GeneratedTemplate {
        frontmatter: frontmatter.to_string(),
        body: body.to_string(),
    })
}

fn run_ai_generate_with_provider(
    llm: &dyn LlmClient,
    _provider: &str,
    model: &str,
) -> Result<GeneratedTemplate, anyhow::Error> {
    let response = llm.generate("test", model)?;
    let parts: Vec<&str> = response.split("---").collect();

    Ok(GeneratedTemplate {
        frontmatter: parts[1].trim().to_string(),
        body: parts.get(2).map(|s| s.trim().to_string()).unwrap_or_default(),
    })
}

fn run_ai_generate_with_tracing(
    llm: &dyn LlmClient,
    fs: &dyn Filesystem,
    tracer: &otel::MockTracerProvider,
    description: &str,
) -> Result<GeneratedTemplate, anyhow::Error> {
    let result = run_ai_generate_command(llm, fs, description, "gpt-4o", "out.tmpl")?;

    let span = otel::MockSpan {
        name: "ggen.ai.generate".to_string(),
        attributes: vec![
            ("ai.provider".to_string(), "openai".to_string()),
            ("ai.model".to_string(), "gpt-4o".to_string()),
            ("description".to_string(), description.to_string()),
        ],
        events: vec!["template_generated".to_string()],
        status: otel::SpanStatus::Ok,
    };
    tracer.record_span(span);

    Ok(result)
}
