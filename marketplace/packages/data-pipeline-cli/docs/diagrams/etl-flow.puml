@startuml etl-flow
!theme blueprint

title ETL Pipeline Flow - Extract, Transform, Load

participant "Pipeline\nOrchestrator" as Orchestrator
participant "Source\nConnector" as Source
participant "Transform\nEngine" as Transform
participant "Sink\nConnector" as Sink
participant "Checkpoint\nManager" as Checkpoint
participant "Metrics\nCollector" as Metrics

box "Extract Phase" #LightBlue
    participant Source
end box

box "Transform Phase" #LightGreen
    participant Transform
end box

box "Load Phase" #LightYellow
    participant Sink
end box

activate Orchestrator

== Pipeline Initialization ==

Orchestrator -> Orchestrator: Load pipeline definition
Orchestrator -> Source: Initialize source connector
activate Source
Source --> Orchestrator: Ready
Orchestrator -> Transform: Initialize transform engine
activate Transform
Transform --> Orchestrator: Ready
Orchestrator -> Sink: Initialize sink connector
activate Sink
Sink --> Orchestrator: Ready

Orchestrator -> Checkpoint: Load checkpoint state
activate Checkpoint
Checkpoint --> Orchestrator: Last successful position

== Extract Phase ==

loop For each batch
    Orchestrator -> Source: Extract batch (size: N)
    Source -> Source: Read from source
    Source -> Source: Parse & validate
    Source --> Orchestrator: Batch[N records]

    Orchestrator -> Metrics: Record extraction metrics
    activate Metrics
    Metrics -> Metrics: Update throughput
    Metrics -> Metrics: Track latency
    deactivate Metrics

    == Transform Phase ==

    Orchestrator -> Transform: Transform batch

    Transform -> Transform: Apply mappings
    note right
        Map source fields to target schema
        - firstName -> foaf:givenName
        - email -> foaf:mbox
    end note

    Transform -> Transform: Apply filters
    note right
        Filter based on conditions
        - status == 'active'
        - age >= 18
    end note

    Transform -> Transform: Apply aggregations
    note right
        Group and aggregate
        - GROUP BY region
        - SUM(amount)
    end note

    Transform -> Transform: Validate data
    note right
        Ensure data quality
        - Email format valid
        - Required fields present
    end note

    Transform --> Orchestrator: Transformed batch

    Orchestrator -> Metrics: Record transform metrics
    activate Metrics
    Metrics -> Metrics: Update transform time
    Metrics -> Metrics: Track quality score
    deactivate Metrics

    == Load Phase ==

    Orchestrator -> Sink: Write batch
    Sink -> Sink: Buffer batch
    Sink -> Sink: Write to sink
    Sink --> Orchestrator: Write confirmation

    Orchestrator -> Metrics: Record load metrics
    activate Metrics
    Metrics -> Metrics: Update write throughput
    Metrics -> Metrics: Track errors
    deactivate Metrics

    == Checkpoint ==

    Orchestrator -> Checkpoint: Save checkpoint
    Checkpoint -> Checkpoint: Persist state
    Checkpoint --> Orchestrator: Checkpoint saved

    alt If errors detected
        Orchestrator -> Orchestrator: Check error threshold
        alt Errors exceed threshold
            Orchestrator -> Orchestrator: Pause pipeline
            Orchestrator -> Metrics: Log critical error
        else Errors within threshold
            Orchestrator -> Orchestrator: Continue processing
            Orchestrator -> Metrics: Log recoverable error
        end
    end
end

== Pipeline Completion ==

Orchestrator -> Source: Close source
deactivate Source
Orchestrator -> Transform: Close transform engine
deactivate Transform
Orchestrator -> Sink: Flush remaining data
Sink -> Sink: Write final batch
Sink --> Orchestrator: Flush complete
deactivate Sink

Orchestrator -> Checkpoint: Save final checkpoint
Checkpoint --> Orchestrator: Final state saved
deactivate Checkpoint

Orchestrator -> Metrics: Generate final report
activate Metrics
Metrics -> Metrics: Calculate totals
Metrics -> Metrics: Compute statistics
Metrics --> Orchestrator: Final metrics
deactivate Metrics

Orchestrator -> Orchestrator: Mark pipeline complete

deactivate Orchestrator

note over Orchestrator
    **Pipeline Metrics:**
    - Total records: 1,000,000
    - Successful: 998,500
    - Filtered: 1,200
    - Errors: 300
    - Duration: 120 seconds
    - Throughput: 8,321 records/sec
    - Avg latency: 12ms
end note

@enduml
