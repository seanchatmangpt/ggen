@startuml data-pipeline-architecture
!theme blueprint

title Data Pipeline CLI - System Architecture

skinparam componentStyle rectangle
skinparam linetype ortho

package "Data Pipeline CLI" {

    component "CLI Interface" as CLI {
        [Command Parser]
        [Argument Validator]
        [Help Generator]
    }

    component "Pipeline Engine" as Engine {
        [Pipeline Orchestrator]
        [Execution Scheduler]
        [Task Manager]
        [Checkpoint Manager]
    }

    component "Source Connectors" as Sources {
        [RDF Source]
        [CSV Source]
        [JSON Source]
        [SQL Source]
        [API Source]
        [Stream Source]
    }

    component "Transform Engine" as Transforms {
        [Mapper]
        [Filter]
        [Aggregator]
        [Joiner]
        [Validator]
    }

    component "Sink Connectors" as Sinks {
        [RDF Sink]
        [CSV Sink]
        [JSON Sink]
        [SQL Sink]
        [API Sink]
        [Stream Sink]
    }

    component "Monitoring & Metrics" as Monitor {
        [Metrics Collector]
        [Performance Tracker]
        [Error Handler]
        [Alert Manager]
    }

    database "Metadata Store" as Meta {
        [Pipeline Definitions]
        [Source Registry]
        [Transform Registry]
        [Sink Registry]
        [Execution History]
    }

    database "Checkpoint Store" as Checkpoint {
        [State Snapshots]
        [Recovery Points]
    }
}

cloud "External Data Sources" as ExtSources {
    database "RDF Stores" as RDFStore
    database "SQL Databases" as SQLStore
    file "CSV Files" as CSVFile
    file "JSON Files" as JSONFile
    interface "REST APIs" as API
    queue "Event Streams" as Stream
}

cloud "External Data Sinks" as ExtSinks {
    database "RDF Stores" as RDFOut
    database "SQL Databases" as SQLOut
    file "Output Files" as FileOut
    interface "APIs" as APIOut
    queue "Message Queues" as QueueOut
}

' CLI to Engine
[Command Parser] --> [Pipeline Orchestrator]
[Argument Validator] --> [Task Manager]

' Engine to Components
[Pipeline Orchestrator] --> [Source Connectors]
[Pipeline Orchestrator] --> [Transform Engine]
[Pipeline Orchestrator] --> [Sink Connectors]
[Execution Scheduler] --> [Task Manager]
[Task Manager] --> [Checkpoint Manager]

' Sources to External
[RDF Source] --> RDFStore
[CSV Source] --> CSVFile
[JSON Source] --> JSONFile
[SQL Source] --> SQLStore
[API Source] --> API
[Stream Source] --> Stream

' Transforms (internal flow)
[Mapper] ..> [Filter]
[Filter] ..> [Aggregator]
[Aggregator] ..> [Joiner]
[Joiner] ..> [Validator]

' Sinks to External
[RDF Sink] --> RDFOut
[CSV Sink] --> FileOut
[JSON Sink] --> FileOut
[SQL Sink] --> SQLOut
[API Sink] --> APIOut
[Stream Sink] --> QueueOut

' Monitoring
[Pipeline Orchestrator] ..> [Metrics Collector]
[Task Manager] ..> [Performance Tracker]
[Checkpoint Manager] ..> [Error Handler]

' Metadata
[Pipeline Orchestrator] --> Meta
[Checkpoint Manager] --> Checkpoint

note right of Engine
  **Pipeline Execution Flow:**
  1. Load pipeline definition
  2. Initialize sources
  3. Start data extraction
  4. Apply transformations
  5. Write to sinks
  6. Update checkpoints
  7. Collect metrics
end note

note bottom of Transforms
  **Transformation Operations:**
  - Map: Field mapping & type conversion
  - Filter: Conditional filtering
  - Aggregate: Group-by & aggregation
  - Join: Multi-source joins
  - Validate: Data quality checks
end note

note bottom of Monitor
  **Monitoring Metrics:**
  - Throughput (records/sec)
  - Latency (end-to-end)
  - Error rate
  - Resource usage
  - Checkpoint frequency
end note

@enduml
