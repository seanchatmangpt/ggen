[package]
name = "ggen-ai"
version = "5.1.0"
edition = "2021"
authors = ["Sean Chatman <sean@chatmangpt.com>"]
description = "Thin wrapper around genai for ggen - LLM integration with environment support"
license = "MIT"
repository = "https://github.com/seanchatmangpt/ggen"
homepage = "https://github.com/seanchatmangpt/ggen"
keywords = ["cli", "code-generation", "rdf", "templates", "llm"]
categories = ["development-tools", "command-line-utilities"]

[dependencies]
# Core ggen dependencies (use workspace = true pattern)
ggen-core.workspace = true
ggen-utils.workspace = true

# HTTP client and async runtime
tokio = { workspace = true }
async-trait = { workspace = true }
reqwest = { workspace = true }

# Serialization
serde = { workspace = true }
serde_json = { workspace = true }

# Time handling
chrono = { workspace = true }

# UUID generation
uuid = { workspace = true }

# Configuration and environment
dotenvy = "0.15"

# Error handling
thiserror = { workspace = true }

# Streaming and futures
futures = { workspace = true }

# Advanced concurrency primitives for hyper-concurrent agent execution
crossbeam = "0.8"           # Lock-free data structures
dashmap = "6.1"             # Concurrent hashmap
parking_lot = "0.12"        # Faster mutexes
flume = "0.11"              # Fast multi-producer channels
arc-swap = "1.7"            # Atomic Arc swapping
governor = "0.8"            # Rate limiting for backpressure

# Logging
tracing = { workspace = true }
tracing-subscriber = { workspace = true }
log = { workspace = true }
once_cell = { workspace = true }

# Template engine (for prompt templating in generators)
tera = { workspace = true }

# Regular expressions for validation
regex = { workspace = true }

# Multi-provider AI client
genai = "0.4"

# LLM Response Caching
moka = { version = "0.12", features = ["future"] }

# SPARQL support
oxigraph = "0.5"

# Crypto for caching
sha2 = "0.10"

[features]
default = []
ollama-integration = []
openai-integration = []
anthropic-integration = []
live-llm-tests = []
all-integrations = [
  "ollama-integration",
  "openai-integration",
  "anthropic-integration",
  "live-llm-tests",
]

[dev-dependencies]
tokio-test = "0.4"
tempfile = "3"
testcontainers = "0.25"
rand = "0.9"
chicago-tdd-tools = { version = "1.4.0", features = [
  "testing-extras",
  "async",
] }
criterion = { version = "0.7", features = ["html_reports"] }

[[bench]]
name = "swarm_coordination"
harness = false

[lib]
name = "ggen_ai"
path = "src/lib.rs"
