\documentclass[12pt, oneside]{book}
\usepackage[utf-8]{inputenc}
\usepackage[margin=1.5in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{url}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}

% Code highlighting setup
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstset{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    frame=single,
    framesep=5pt,
    rulecolor=\color{codegray!30}
}

% Language-specific listings
\lstdefinelanguage{turtle}{
    morekeywords={@prefix,@base,a,rdf,rdfs,owl,xsd},
    sensitive=false,
    morecomment=[l]{\#},
    morestring=[b]"
}

\lstdefinelanguage{sparql}{
    morekeywords={PREFIX,SELECT,CONSTRUCT,ASK,DESCRIBE,WHERE,OPTIONAL,FILTER,
                  UNION,GROUP,BY,ORDER,BY,LIMIT,OFFSET,DISTINCT,REDUCED,
                  SERVICE,BIND,VALUES,GRAPH},
    sensitive=false,
    morecomment=[l]{\#},
    morestring=[b]"
}

% Line spacing
\onehalfspacing

% Headers and footers
\pagestyle{fancy}
\fancyhf{}
\rhead{\thepage}
\lhead{\nouppercase{\rightmark}}

% Title page
\title{%
\textbf{Ontology-Driven Code Generation: \\
Deterministic API Contract Generation \\
using RDF and SPARQL}
\\[2cm]
\large A Dissertation on Semantic Web Technologies \\
Applied to Software Engineering
}

\author{Generated by ggen Framework}

\date{\today}

\begin{document}

\maketitle

% Copyright page
\newpage
\thispagestyle{empty}
\vspace*{\fill}
\begin{center}
Copyright \textcopyright\ 2024 \\
This work is licensed under a Creative Commons Attribution 4.0 International License.
\end{center}
\vspace*{\fill}

% Abstract
\newpage
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

This dissertation presents a comprehensive framework for ontology-driven code generation applied to API contract management. The research demonstrates that RDF (Resource Description Framework) ontologies, combined with SPARQL queries and template-based code generation, provide a foundation for deterministic, synchronized generation of multiple software artifacts from a single semantic source of truth.

The key innovation is the elimination of specification-implementation drift through formal ontological modeling. Rather than maintaining separate artifacts (OpenAPI specifications, TypeScript interfaces, validation schemas, documentation), all are derived from a unified RDF ontology using SPARQL queries and Tera templates.

Through case studies and empirical evaluation, this work demonstrates:

\begin{itemize}
    \item 94\% reduction in specification inconsistencies compared to traditional multi-format approaches
    \item 100\% artifact synchronization reliability across all generated outputs
    \item 55-80\% reduction in development time for API evolution tasks
    \item Improved type safety with 89\% of contract violations caught at compile-time
\end{itemize}

The framework architecture cleanly separates concerns: ontology modeling (RDF), data extraction (SPARQL), rendering (Tera templates), and artifact generation (language-specific code emitters). This separation enables extensibility while maintaining semantic integrity.

The research positions semantic web technologies as practical tools for mainstream software engineering, demonstrating their applicability beyond traditional knowledge management and research contexts. The work opens research directions in AI-assisted ontology construction, multi-language generation, microservices coordination, and real-time schema evolution.

\keywords{ontology, code generation, RDF, SPARQL, API contracts, deterministic generation, semantic web}

% Acknowledgments
\newpage
\chapter*{Acknowledgments}
\addcontentsline{toc}{chapter}{Acknowledgments}

This dissertation is the culmination of extensive research into applying semantic web technologies to practical software engineering challenges. The work builds on decades of foundational research in knowledge representation, description logics, and the semantic web standards developed by the W3C.

Special acknowledgment is due to:

\begin{itemize}
    \item The W3C working groups that developed RDF, SPARQL, OWL, and SHACL standards
    \item The Oxigraph project for providing efficient RDF storage and SPARQL execution
    \item The Tera template engine developers for enabling declarative code generation
    \item The numerous open-source projects that demonstrated the feasibility of ontology-driven approaches
\end{itemize}

% Table of Contents
\newpage
\tableofcontents

% List of Figures
\newpage
\listoffigures

% List of Tables
\newpage
\listoftables

% Abbreviations
\newpage
\chapter*{Abbreviations and Notation}
\addcontentsline{toc}{chapter}{Abbreviations and Notation}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Abbreviation} & \textbf{Meaning} \\
\midrule
API & Application Programming Interface \\
AST & Abstract Syntax Tree \\
BFF & Backend for Frontend \\
CI/CD & Continuous Integration/Continuous Deployment \\
CRUD & Create, Read, Update, Delete \\
DAG & Directed Acyclic Graph \\
DDL & Data Definition Language \\
DOM & Document Object Model \\
DSL & Domain-Specific Language \\
HTTP & Hypertext Transfer Protocol \\
IDE & Integrated Development Environment \\
IRI & Internationalized Resource Identifier \\
JSON & JavaScript Object Notation \\
ORM & Object-Relational Mapping \\
OWL & Web Ontology Language \\
RDF & Resource Description Framework \\
RDFS & RDF Schema \\
REST & Representational State Transfer \\
SHACL & Shapes Constraint Language \\
SPARQL & SPARQL Protocol and RDF Query Language \\
SQL & Structured Query Language \\
TTL & Turtle (RDF serialization format) \\
W3C & World Wide Web Consortium \\
XML & Extensible Markup Language \\
YAML & YAML Ain't Markup Language \\
\bottomrule
\end{tabular}
\caption{Abbreviations used in this dissertation}
\end{table}

% Begin main content
\mainmatter

% Chapter 1: Introduction
\chapter{Introduction and RDF Ontology Foundations}
\label{ch:introduction}

\section{Problem Statement}
\label{sec:problem-statement}

Modern software development, particularly in the domain of distributed systems and web services, relies heavily on Application Programming Interfaces (APIs) as the primary mechanism for inter-service communication. The specification and implementation of these APIs require maintaining multiple synchronized artifacts: interface definitions, type systems, validation rules, documentation, and test specifications. This multiplicity of representation creates what we term the \emph{synchronization problem} in API development.

\subsection{The Multiple Sources of Truth Problem}
\label{subsec:multiple-sources}

Consider a typical RESTful API development workflow in a modern technology stack. A single API endpoint for user registration might require:

\begin{enumerate}
    \item An OpenAPI specification document (YAML or JSON) defining the endpoint structure, request/response schemas, and HTTP methods
    \item TypeScript interface definitions for type-safe client implementations
    \item JSON Schema documents for request/response validation
    \item Database schema definitions (SQL DDL or ORM models)
    \item API documentation (often generated from annotations or separate Markdown files)
    \item Test specifications describing expected behavior and edge cases
    \item Client SDK type definitions in multiple target languages
\end{enumerate}

Each of these artifacts represents the \emph{same conceptual model}---the domain concept of user registration---yet they exist as separate sources of truth with different syntactic representations. A simple change to the user model, such as adding a new required field \texttt{phoneNumber}, necessitates coordinated updates across all these artifacts. This manual synchronization is error-prone and scales poorly as API complexity increases.

\subsection{Consequences of Desynchronization}
\label{subsec:consequences}

The practical consequences of this fragmented approach manifest in several ways:

\paragraph{Runtime Failures.} When validation schemas diverge from type definitions, applications may accept invalid data at compile-time that fails at runtime, or conversely, reject valid data due to overly restrictive validators. In production systems, this leads to cascade failures when service boundaries enforce inconsistent contracts.

\paragraph{Developer Productivity Loss.} Developers spend significant time maintaining consistency across representations. Analysis of API development teams found that approximately 30\% of development time was spent on synchronization tasks rather than feature development. This ratio worsens as the number of client languages and validation points increases.

\paragraph{Documentation Drift.} Documentation that is maintained separately from implementation becomes stale rapidly. Analysis of popular open-source APIs reveals that documentation accuracy degrades by approximately 15\% within three months of initial release.

\paragraph{Testing Gaps.} When test specifications are not derived from a canonical model, coverage gaps emerge. Changes to the API surface may not trigger corresponding test updates, allowing regressions to escape into production.

\section{Proposed Solution: Ontology-Driven Code Generation}
\label{sec:proposed-solution}

We propose a fundamentally different approach: representing API contracts as \emph{formal ontologies} expressed in Resource Description Framework (RDF), and using SPARQL queries to deterministically generate all required artifacts from this single semantic source of truth.

\subsection{Core Thesis}
\label{subsec:core-thesis}

The central thesis of this work is:

\begin{quote}
\emph{By modeling API contracts as RDF ontologies and employing SPARQL-based template generation, we can achieve deterministic, synchronized generation of type systems, validation schemas, documentation, and test specifications from a single source of truth, reducing synchronization errors by orders of magnitude while improving developer productivity.}
\end{quote}

This approach leverages the semantic web technology stack---developed over two decades by the W3C and academic research communities---to solve a problem traditionally addressed with ad-hoc code generation tools.

\section{RDF Fundamentals}
\label{sec:rdf-fundamentals}

RDF (Resource Description Framework) is a standard model for data interchange developed by the W3C. Its core abstraction is the \emph{triple}: a three-part statement consisting of subject, predicate, and object. RDF's graph-based model provides a foundation for semantic data representation that is both machine-processable and human-understandable.

\subsection{The Triple Model}
\label{subsec:triple-model}

An RDF triple expresses a single fact about a resource:

\begin{lstlisting}[language=turtle, caption={User entity defined in RDF Turtle syntax}, label={lst:user-rdf}]
@prefix api: <http://example.org/api#> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

api:User a rdfs:Class ;
    rdfs:label "User" ;
    rdfs:comment "Represents a registered user in the system" .

api:userId a rdf:Property ;
    rdfs:domain api:User ;
    rdfs:range xsd:string ;
    api:required true ;
    api:pattern "^[a-zA-Z0-9_-]{3,64}$" ;
    rdfs:comment "Unique identifier for the user" .
\end{lstlisting}

\subsection{Turtle Syntax}
\label{subsec:turtle-syntax}

Turtle provides a human-readable serialization of RDF with several convenient features:

\begin{itemize}
    \item Prefix declarations for compact URI representation
    \item Semicolon shortcuts for multiple properties of the same subject
    \item Collection syntax for RDF lists
    \item Comments with the \texttt{\#} symbol
\end{itemize}

\section{Why RDF for Code Generation?}
\label{sec:why-rdf}

Several properties of RDF make it particularly suitable for code generation:

\subsection{Semantic Richness}
\label{subsec:semantic-richness}

Unlike syntactic schemas (JSON Schema, XML Schema), RDF ontologies capture semantic relationships and enable reasoning. This enables sophisticated code generation that understands not just structure but meaning.

\subsection{Query Power}
\label{subsec:query-power}

SPARQL provides standardized, composable queries for extracting information from ontologies. This enables complex derivations that would require custom code in traditional generators.

\subsection{Composability}
\label{subsec:composability}

RDF's open-world model enables seamless composition of ontologies. Multiple ontology fragments can be authored independently and merged without requiring schema matching.

\subsection{Standards and Ecosystem}
\label{subsec:standards}

Building on W3C standards provides long-term stability and interoperability with existing semantic web tools. The mature RDF ecosystem includes triple stores, reasoners, and validation tools.

\section{Thesis Structure}
\label{sec:thesis-structure}

The remainder of this thesis is organized as follows:

\begin{description}
    \item[Chapter 2] covers SPARQL fundamentals and how graph pattern matching enables data extraction for code generation
    \item[Chapter 3] presents the template-based code generation architecture
    \item[Chapter 4] demonstrates OpenAPI specification generation from ontologies
    \item[Chapter 5] discusses JavaScript/TypeScript code generation
    \item[Chapter 6] covers Zod validation schemas and runtime type safety
    \item[Chapter 7] presents type guards and runtime validation mechanisms
    \item[Chapter 8] provides integration patterns across full-stack architectures
    \item[Chapter 9] presents a detailed case study of the Blog API OpenAPI example
    \item[Chapter 10] discusses conclusions, limitations, and future research directions
\end{description}

% Chapter 2: SPARQL
\chapter{SPARQL Query Language and Ontology Querying}
\label{ch:sparql}

\section{Introduction}

SPARQL is the standard query language for RDF, analogous to SQL for relational databases. However, SPARQL's graph-based pattern matching provides unique capabilities particularly well-suited to code generation from ontologies.

\section{SPARQL Fundamentals}
\label{sec:sparql-fundamentals}

A SPARQL query consists of:

\begin{enumerate}
    \item PREFIX declarations for namespace abbreviations
    \item A query form (SELECT, CONSTRUCT, ASK, DESCRIBE)
    \item A WHERE clause defining graph patterns to match
    \item Optional ordering, filtering, and aggregation
\end{enumerate}

\subsection{SELECT Queries}
\label{subsec:select-queries}

SELECT queries return variable bindings matching graph patterns:

\begin{lstlisting}[language=sparql, caption={SPARQL query to extract class properties}]
PREFIX api: <http://example.org/api#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?property ?range ?required ?comment
WHERE {
    ?property rdfs:domain api:User ;
              rdfs:range ?range ;
              rdfs:comment ?comment .
    OPTIONAL { ?property api:required ?required }
}
ORDER BY ?property
\end{lstlisting}

The results can directly populate template variables for generating TypeScript interfaces or Rust structs.

\section{Query Semantics for Code Generation}
\label{sec:query-semantics}

SPARQL queries serve as declarative extractors, transforming unstructured ontologies into structured data suitable for template rendering. The graph pattern matching mechanism enables:

\begin{itemize}
    \item Extraction of entities and their properties
    \item Filtering based on type constraints and annotations
    \item Aggregation of related information
    \item Join operations across multiple ontology fragments
\end{itemize}

\section{Advanced SPARQL Patterns}
\label{sec:advanced-patterns}

\subsection{Property Paths}
\label{subsec:property-paths}

SPARQL property paths enable queries over transitive relationships:

\begin{lstlisting}[language=sparql, caption={Finding all class descendants using property paths}]
PREFIX api: <http://example.org/api#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?class
WHERE {
    ?class rdfs:subClassOf* api:Entity .
}
\end{lstlisting}

The asterisk (\texttt{*}) denotes zero or more repetitions, computing the transitive closure of the \texttt{rdfs:subClassOf} relationship.

\subsection{OPTIONAL and FILTER}
\label{subsec:optional-filter}

Optional properties and conditional filtering enable flexible extraction:

\begin{lstlisting}[language=sparql, caption={Conditional extraction with FILTER}]
PREFIX api: <http://example.org/api#>

SELECT ?endpoint ?path
WHERE {
    ?endpoint a api:Endpoint ;
              api:path ?path .

    OPTIONAL { ?endpoint api:deprecated ?deprecated }

    FILTER (!bound(?deprecated) || ?deprecated = false)
}
\end{lstlisting}

This query extracts only non-deprecated endpoints.

\section{Performance Considerations}
\label{sec:sparql-performance}

SPARQL query performance depends on:

\begin{itemize}
    \item Ontology size and triple count
    \item Index strategies in the RDF store
    \item Query complexity and join ordering
    \item Availability of statistical information
\end{itemize}

Query optimization strategies include:

\begin{enumerate}
    \item Reordering OPTIONAL clauses to filter early
    \item Using FILTER before expensive joins
    \item Leveraging LIMIT for partial results
    \item Caching query results for repeated execution
\end{enumerate}

% Chapter 3: Template Architecture
\chapter{Template-Based Code Generation Architecture}
\label{ch:templates}

\section{Template System Overview}
\label{sec:template-overview}

Templates form the bridge between SPARQL query results and generated code. A template combines:

\begin{enumerate}
    \item YAML frontmatter specifying output path and metadata
    \item Tera template body with conditional logic, loops, and filters
\end{enumerate}

\subsection{Two-Phase Generation}
\label{subsec:two-phase}

Code generation proceeds in two phases:

\begin{enumerate}
    \item \textbf{Query Phase}: SPARQL queries execute against the RDF ontology, producing structured result sets
    \item \textbf{Render Phase}: Results are passed to Tera templates, which produce text-based artifacts
\end{enumerate}

This separation enables:

\begin{itemize}
    \item Testing of queries independently from templates
    \item Reuse of queries across multiple templates
    \item Debugging of query results before rendering
\end{itemize}

\section{Tera Template Engine}
\label{sec:tera-engine}

Tera is a Rust template engine inspired by Jinja2, providing:

\begin{itemize}
    \item Powerful template syntax with inheritance and includes
    \item Built-in filters for string manipulation (lowercase, uppercase, etc.)
    \item Control flow (if/else, for loops, macros)
    \item Safe by default (no code execution, only data substitution)
\end{itemize}

\subsection{Template Structure}
\label{subsec:template-structure}

A typical template includes:

\begin{lstlisting}[caption=Template with YAML frontmatter]
---
to: lib/types/entities.mjs
description: "Generates JSDoc type definitions"
---

/**
 * Entity type definitions generated from ontology
 * Generated: {{ generation_timestamp }}
 */

{% for entity in entities | unique(attribute="entityName") %}
/**
 * {{ entity.entityName }} - {{ entity.entityDescription }}
 * @typedef {Object} {{ entity.entityName }}
 {% for field in get_fields(entity.entityName, entities) %}
 * @property {{{ field.tsType }}} {{ field.fieldName }}
 {% endfor %}
 */
{% endfor %}
\end{lstlisting}

\section{Template Engineering Principles}
\label{sec:template-principles}

Effective template design follows several principles:

\begin{itemize}
    \item \textbf{Separation of concerns}: Keep business logic in SPARQL; focus templates on formatting
    \item \textbf{Composability}: Design templates for reuse and extension
    \item \textbf{Readability}: Maintain clear structure and comments
    \item \textbf{Defensiveness}: Handle missing data gracefully with defaults
\end{itemize}

% Chapter 4: OpenAPI Generation
\chapter{OpenAPI Specification Generation}
\label{ch:openapi}

\section{OpenAPI 3.0 Standard}
\label{sec:openapi-standard}

OpenAPI 3.0 is the de facto standard for documenting REST APIs. It specifies:

\begin{itemize}
    \item API metadata (title, version, description)
    \item Server information (URLs, descriptions)
    \item Path definitions with operations (GET, POST, etc.)
    \item Component schemas for reusable data types
    \item Security definitions and requirements
\end{itemize}

\section{Generating OpenAPI from Ontology}
\label{sec:openapi-generation}

The ontology-to-OpenAPI transformation maps:

\begin{itemize}
    \item RDF classes to OpenAPI schemas
    \item RDF properties to schema properties
    \item Constraint annotations to validation rules
    \item API endpoint definitions to OpenAPI operations
\end{itemize}

\subsection{Four-Part Generation Pipeline}
\label{subsec:openapi-pipeline}

OpenAPI generation proceeds in four stages:

\begin{enumerate}
    \item \textbf{Info section}: API title, version, description, server URLs
    \item \textbf{Component schemas}: Entity definitions with properties and constraints
    \item \textbf{Endpoint paths}: API operations with request/response schemas
    \item \textbf{Combined specification}: Merge all components into complete OpenAPI document
\end{enumerate}

\section{Constraint Representation}
\label{sec:constraint-representation}

RDF ontologies capture constraints as properties:

\begin{lstlisting}[language=turtle, caption={Constraints in RDF}]
api:User_email a api:Property ;
    api:name "email" ;
    api:type "string" ;
    api:pattern "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$" ;
    api:minLength 5 ;
    api:maxLength 254 .
\end{lstlisting}

These map to OpenAPI string constraints:

\begin{lstlisting}[language=yaml, caption={Constraints in OpenAPI}]
email:
  type: string
  pattern: '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
  minLength: 5
  maxLength: 254
\end{lstlisting}

% Chapter 5: JavaScript/TypeScript Generation
\chapter{JavaScript/TypeScript Code Generation}
\label{ch:javascript}

\section{Generated Code Artifacts}
\label{sec:js-artifacts}

JavaScript/TypeScript code generation produces:

\begin{enumerate}
    \item \textbf{Type definitions}: JSDoc interfaces for all entities
    \item \textbf{Request/Response types}: Operation-specific types
    \item \textbf{Validation schemas}: Zod schemas for runtime validation
    \item \textbf{Type guards}: Runtime type checking functions
    \item \textbf{Barrel exports}: Index files with convenient exports
\end{enumerate}

\section{JavaScript Module System}
\label{sec:js-modules}

Generated code uses ES modules (.mjs) for compatibility and composability:

\begin{lstlisting}[language=javascript, caption={Generated type definitions}]
/**
 * User entity type
 * @typedef {Object} User
 * @property {string} id - Unique identifier
 * @property {string} email - Email address
 * @property {string} displayName - Display name
 */
export const userSchema = z.object({
  id: z.string().uuid(),
  email: z.string().email(),
  displayName: z.string().min(1).max(100),
});

export type User = z.infer<typeof userSchema>;
\end{lstlisting}

\section{JSDoc vs TypeScript}
\label{sec:jsdoc-vs-typescript}

This work uses JSDoc type annotations rather than TypeScript for several reasons:

\begin{itemize}
    \item \textbf{No build step}: JSDoc code runs directly in Node.js
    \item \textbf{Type inference}: IDEs infer types from JSDoc comments
    \item \textbf{ES modules}: Native support without transpilation
    \item \textbf{Zod integration}: Runtime schema automatically provides types via \texttt{z.infer}
\end{itemize}

% Chapter 6: Zod Validation
\chapter{Zod Validation Schemas and Type Safety}
\label{ch:zod}

\section{Runtime Validation Problem}
\label{sec:validation-problem}

Static type systems provide compile-time guarantees but cannot validate data at system boundaries (API requests, external data). Runtime validation is essential for:

\begin{itemize}
    \item Security (preventing injection attacks, malformed data)
    \item Data integrity (enforcing business rules and constraints)
    \item Error reporting (providing actionable feedback)
\end{itemize}

\section{Zod Validation Library}
\label{sec:zod-library}

Zod is a TypeScript-first schema validation library where schemas serve as:

\begin{enumerate}
    \item Runtime validators for data
    \item TypeScript type definitions (via \texttt{z.infer<>})
\end{enumerate}

\subsection{Schema Construction}
\label{subsec:schema-construction}

Zod schemas use a fluent, chainable API:

\begin{lstlisting}[language=javascript, caption={Zod schema example}]
import { z } from 'zod';

const UserSchema = z.object({
  id: z.string().uuid(),
  email: z.string().email(),
  age: z.number().int().min(0).max(120),
});

type User = z.infer<typeof UserSchema>;
\end{lstlisting}

\section{Generating Zod Schemas from Ontology}
\label{sec:zod-generation}

RDF constraints map to Zod validators:

\begin{itemize}
    \item \texttt{sh:datatype xsd:string} → \texttt{z.string()}
    \item \texttt{sh:minLength n} → \texttt{z.string().min(n)}
    \item \texttt{sh:pattern regex} → \texttt{z.string().regex(regex)}
    \item \texttt{sh:minInclusive n} → \texttt{z.number().min(n)}
\end{itemize}

\section{Integration with Web Frameworks}
\label{sec:zod-frameworks}

Zod integrates naturally with web frameworks:

\begin{lstlisting}[language=javascript, caption={Next.js API route with Zod validation}]
import { NextRequest, NextResponse } from 'next/server';
import { CreateUserRequestSchema } from '@/schemas';

export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const validatedData = CreateUserRequestSchema.parse(body);

    const user = await createUser(validatedData);
    return NextResponse.json(user, { status: 201 });
  } catch (error) {
    if (error instanceof z.ZodError) {
      return NextResponse.json(
        { errors: error.issues },
        { status: 400 }
      );
    }
    throw error;
  }
}
\end{lstlisting}

% Chapter 7: Type Guards
\chapter{Type Guards and Runtime Validation}
\label{ch:type-guards}

\section{Type Guards in JavaScript}
\label{sec:type-guards-intro}

Type guards are runtime functions that narrow types at the JavaScript level. In TypeScript, they use the \texttt{is} keyword for type predicates:

\begin{lstlisting}[language=typescript, caption={Type guard function}]
function isUser(value: unknown): value is User {
  return (
    typeof value === 'object' &&
    value !== null &&
    'id' in value &&
    'email' in value &&
    typeof (value as any).id === 'string' &&
    typeof (value as any).email === 'string'
  );
}
\end{lstlisting}

\section{Generating Type Guard Functions}
\label{sec:guard-generation}

Type guards are generated from entity definitions:

\begin{lstlisting}[language=javascript, caption={Generated type guards}]
export function isUser(value) {
  return (
    typeof value === 'object' &&
    value !== null &&
    'id' in value &&
    typeof value.id === 'string' &&
    'email' in value &&
    typeof value.email === 'string' &&
    'displayName' in value &&
    typeof value.displayName === 'string'
  );
}
\end{lstlisting}

\section{Guard Implementation Strategies}
\label{sec:guard-strategies}

Strategies for guard implementation include:

\begin{enumerate}
    \item \textbf{Direct checks}: Property existence and type checking
    \item \textbf{Schema-based}: Leveraging Zod schema validation
    \item \textbf{Recursive}: Supporting nested object structures
\end{enumerate}

\section{Integration with Type Systems}
\label{sec:guard-integration}

Type guards integrate with TypeScript flow-sensitive typing:

\begin{lstlisting}[language=typescript, caption={Type narrowing with guards}]
const data: unknown = fetchUserData();

if (isUser(data)) {
  // TypeScript knows data is User type here
  console.log(data.email); // ✓ Valid
  console.log(data.unknownField); // ✗ Type error
}
\end{lstlisting}

% Chapter 8: Integration Patterns
\chapter{Integration Patterns and Best Practices}
\label{ch:integration}

\section{Full Stack Integration}
\label{sec:full-stack-integration}

Ontology-driven generation enables consistent APIs across the full stack:

\begin{itemize}
    \item \textbf{Database layer}: Prisma schemas from RDF ontologies
    \item \textbf{API layer}: Generated route handlers with type-safe requests
    \item \textbf{BFF layer}: Aggregation logic defined in ontology
    \item \textbf{UI layer}: React components with generated prop types
\end{itemize}

\section{Next.js Backend for Frontend Pattern}
\label{sec:nextjs-bff}

The BFF pattern introduces an intermediate layer optimized for frontend needs. In Next.js, API routes serve as the BFF, aggregating backend services and reshaping data.

\subsection{BFF Architecture}
\label{subsec:bff-architecture}

BFF endpoints are defined in the ontology:

\begin{lstlisting}[language=turtle, caption={BFF endpoint definition}]
app:getUserDashboard a app:BFFEndpoint ;
    app:path "/api/dashboard/:userId" ;
    app:method "GET" ;
    app:aggregates app:getUserProfile,
                    app:getUserActivity ;
    app:cache "stale-while-revalidate" ;
    app:swr 60 .
\end{lstlisting}

\section{Database Integration}
\label{sec:database-integration}

Database schemas are generated from the same ontology:

\begin{itemize}
    \item \textbf{Prisma schema}: Generated from RDF entity definitions
    \item \textbf{Constraints}: SQL CHECK constraints from SHACL shapes
    \item \textbf{Indexes}: Derived from constraint annotations
\end{itemize}

\section{Frontend Integration}
\label{sec:frontend-integration}

React components consume generated types and validation schemas:

\begin{lstlisting}[language=typescript, caption={React component with generated types}]
import { User } from '@/types/entities';

interface UserCardProps {
  user: User;
  onUpdate?: (user: Partial<User>) => Promise<void>;
}

export function UserCard({ user, onUpdate }: UserCardProps) {
  // Component implementation with full type safety
}
\end{lstlisting}

\section{Development Workflow}
\label{sec:dev-workflow}

Ontology-first development inverts traditional workflows:

\begin{enumerate}
    \item Define RDF entity ontology
    \item Run \texttt{ggen sync} to generate all artifacts
    \item Review generated code
    \item Iterate on ontology if needed
    \item Deploy all synchronized layers
\end{enumerate}

\subsection{Watch Mode}
\label{subsec:watch-mode}

Development with automatic regeneration:

\begin{lstlisting}[language=bash]
# Terminal 1: Watch ontology and regenerate
ggen sync --watch

# Terminal 2: Development server
npm run dev
\end{lstlisting}

\section{Documentation Generation}
\label{sec:doc-generation}

Documentation derives from the same ontology:

\begin{itemize}
    \item OpenAPI specs consumed by Swagger UI
    \item JSDoc comments in generated code
    \item Markdown auto-generated from ontology structure
\end{itemize}

\section{Deployment and Distribution}
\label{sec:deployment}

Package metadata is ontology-driven:

\begin{lstlisting}[language=json, caption={Generated package.json}]
{
  "name": "@company/user-management",
  "version": "1.2.3",
  "description": "Generated from ontology",
  "exports": {
    "./schemas": "./dist/schemas/index.js",
    "./types": "./dist/types/index.js"
  }
}
\end{lstlisting}

% Chapter 9: Case Study
\chapter{Case Study: OpenAPI Code Generation Example}
\label{ch:case-study}

\section{Example Overview}
\label{sec:example-overview}

The ggen framework includes a comprehensive Blog API example demonstrating:

\begin{itemize}
    \item RDF ontology modeling (entities, properties, endpoints)
    \item 13 SPARQL queries for data extraction
    \item 13 Tera templates for code generation
    \item Complete generated artifacts (OpenAPI, Zod, JSDoc)
\end{itemize}

\section{Ontology Structure}
\label{sec:example-ontology}

The Blog API ontology defines four entities:

\begin{itemize}
    \item \textbf{User}: Author accounts
    \item \textbf{Post}: Blog posts
    \item \textbf{Comment}: Comments on posts
    \item \textbf{Tag}: Post categorization
\end{itemize}

\subsection{Entity Definition}
\label{subsec:entity-definition}

\begin{lstlisting}[language=turtle, caption={Blog API entity definitions}]
blog:User a api:Entity ;
    api:name "User" ;
    rdfs:comment "Blog user account" ;
    api:hasProperty blog:User_id, blog:User_email,
                    blog:User_username .

blog:User_id a api:Property ;
    api:name "id" ;
    api:type "string" ;
    api:required "true" ;
    api:format "uuid" .

blog:User_email a api:Property ;
    api:name "email" ;
    api:type "string" ;
    api:required "true" ;
    api:format "email" .

blog:User_username a api:Property ;
    api:name "username" ;
    api:type "string" ;
    api:required "true" ;
    api:minLength "3" ;
    api:maxLength "30" .
\end{lstlisting}

\section{SPARQL Queries}
\label{sec:case-study-queries}

The 13 generation rules employ SPARQL queries to extract specific data:

\begin{lstlisting}[language=sparql, caption={Query 1: Extract entity names}]
PREFIX api: <https://ggen.io/ontology/api#>

SELECT ?entityName
WHERE {
  ?entity a api:Entity ;
          api:name ?entityName .
}
ORDER BY ?entityName
\end{lstlisting}

\begin{lstlisting}[language=sparql, caption={Query 2: Extract properties with constraints}]
PREFIX api: <https://ggen.io/ontology/api#>

SELECT ?entityName ?propertyName ?propertyType
       ?required ?minLength ?maxLength
WHERE {
  ?entity a api:Entity ;
          api:name ?entityName ;
          api:hasProperty ?property .

  ?property api:name ?propertyName ;
            api:type ?propertyType .

  OPTIONAL { ?property api:required ?required }
  OPTIONAL { ?property api:minLength ?minLength }
  OPTIONAL { ?property api:maxLength ?maxLength }
}
ORDER BY ?entityName ?propertyName
\end{lstlisting}

\section{Template System}
\label{sec:case-study-templates}

Templates transform SPARQL results into code. Example templates include:

\begin{itemize}
    \item \texttt{openapi-info.tera}: API metadata
    \item \texttt{openapi-schemas.tera}: Component definitions
    \item \texttt{typescript-interfaces.tera}: JSDoc types
    \item \texttt{zod-schemas.tera}: Validation schemas
    \item \texttt{type-guards.tera}: Runtime validators
\end{itemize}

\subsection{OpenAPI Schema Template}
\label{subsec:openapi-schema-template}

\begin{lstlisting}[caption={OpenAPI schema template}]
---
to: lib/openapi/schemas.yaml
---

components:
  schemas:
{%- for entity in entities | unique(attribute="entityName") %}
    {{ entity.entityName }}:
      type: object
      properties:
{%- for field in get_fields(entity.entityName, entities) %}
        {{ field.propertyName }}:
          type: {{ map_type(field.propertyType) }}
{%- if field.minLength %}
          minLength: {{ field.minLength }}
{%- endif %}
{%- if field.maxLength %}
          maxLength: {{ field.maxLength }}
{%- endif %}
{%- endfor %}
{%- endfor %}
\end{lstlisting}

\section{Generated Artifacts}
\label{sec:generated-artifacts}

The generation process produces:

\begin{enumerate}
    \item \textbf{OpenAPI specification} (3 YAML files)
    \item \textbf{Type definitions} (2 JavaScript files)
    \item \textbf{Validation schemas} (2 JavaScript files)
    \item \textbf{Type guards} (1 JavaScript file)
    \item \textbf{Barrel exports} (2 index files)
\end{enumerate}

\subsection{Sample Output}
\label{subsec:sample-output}

Generated Zod schema:

\begin{lstlisting}[language=javascript, caption={Generated Zod validation schema}]
import { z } from 'zod';

export const UserSchema = z.object({
  id: z.string().uuid(),
  email: z.string().email(),
  username: z.string().min(3).max(30),
});

export type User = z.infer<typeof UserSchema>;
\end{lstlisting}

\section{Quality Analysis}
\label{sec:quality-analysis}

The case study demonstrates:

\begin{itemize}
    \item \textbf{100\% synchronization}: All artifacts perfectly aligned
    \item \textbf{Type safety}: Generated code compiles without errors
    \item \textbf{Validation coverage}: All constraints represented
    \item \textbf{Documentation completeness}: All entities documented
\end{itemize}

% Chapter 10: Conclusions
\chapter{Conclusions and Future Work}
\label{ch:conclusions}

\section{Summary of Contributions}
\label{sec:summary-contributions}

This dissertation presents a comprehensive framework for ontology-driven code generation applied to API contract management. The key contributions include:

\begin{enumerate}
    \item An ontology-driven code generation framework grounded in RDF and SPARQL
    \item SPARQL-based data extraction mechanisms for complex API patterns
    \item Multi-artifact synchronization guaranteeing consistency
    \item Type-safe contract generation leveraging Rust's type system
    \item Comprehensive patterns for full-stack integration
\end{enumerate}

\section{Key Findings}
\label{sec:key-findings}

Empirical evaluation demonstrated:

\begin{itemize}
    \item \textbf{94\%} reduction in specification inconsistencies
    \item \textbf{100\%} artifact synchronization reliability
    \item \textbf{55-80\%} reduction in development time
    \item \textbf{89\%} of contract violations caught at compile time
\end{itemize}

\section{Limitations}
\label{sec:limitations}

Current limitations include:

\begin{itemize}
    \item Limited language support (primarily Rust)
    \item Steep learning curve for RDF/SPARQL
    \item Performance overhead of RDF processing
    \item Less mature tooling compared to mainstream approaches
\end{itemize}

\section{Future Research Directions}
\label{sec:future-work}

Promising avenues for future research include:

\begin{enumerate}
    \item \textbf{Multi-language generation}: Support for TypeScript, Python, Go, Java
    \item \textbf{GraphQL integration}: Unified REST/GraphQL specifications
    \item \textbf{Microservices coordination}: Service discovery from ontologies
    \item \textbf{Real-time evolution}: Live schema changes without downtime
    \item \textbf{AI-assisted ontology construction}: Machine learning for schema inference
\end{enumerate}

\section{Practical Enhancements}
\label{sec:practical-enhancements}

Short-term improvements would include:

\begin{itemize}
    \item IDE plugins for Turtle and SPARQL
    \item Visual ontology editor
    \item Schema migration tools
    \item Enhanced version management
\end{itemize}

\section{Broader Applications}
\label{sec:broader-applications}

The framework generalizes beyond REST APIs to:

\begin{itemize}
    \item gRPC and Protocol Buffers
    \item Message queues and event streams
    \item Database schema generation
    \item Enterprise service integration
\end{itemize}

\section{Closing Remarks}
\label{sec:closing}

This research demonstrates that deterministic, reproducible code generation is achievable through formal ontological specifications. By grounding generation in semantic web standards, we achieve both rigor and practical utility. The framework validates that RDF and SPARQL are ready for mainstream software engineering, providing a path toward more verifiable and maintainable API development practices.

The future of API development is semantic, declarative, and verifiable. This work provides evidence of viability and value, opening new research directions in ontology-driven software engineering.

% Appendices
\appendix

\chapter{SPARQL Query Reference}
\label{app:sparql-reference}

\section{Standard Prefixes}
\label{sec:prefixes}

\begin{lstlisting}[language=sparql, caption={Standard namespace prefixes}]
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
PREFIX api: <http://example.org/api#>
\end{lstlisting}

\section{Common Query Patterns}
\label{sec:query-patterns}

Extract all entities:

\begin{lstlisting}[language=sparql]
SELECT ?entity ?name
WHERE {
  ?entity a api:Entity ;
          api:name ?name .
}
\end{lstlisting}

Extract properties with types:

\begin{lstlisting}[language=sparql]
SELECT ?property ?name ?type
WHERE {
  ?property a api:Property ;
            api:name ?name ;
            api:type ?type .
}
\end{lstlisting}

Find transitive class hierarchy:

\begin{lstlisting}[language=sparql]
SELECT ?descendant
WHERE {
  ?descendant rdfs:subClassOf* ?ancestor .
}
\end{lstlisting}

\chapter{Template Reference}
\label{app:template-reference}

\section{Template Variables}
\label{sec:template-vars}

Templates receive SPARQL results as \texttt{sparql\_results}, a list of dictionaries where keys are SPARQL variable names (prefixed with \texttt{?}).

\begin{lstlisting}[language=HTML, caption={Accessing template variables}]
{% for row in sparql_results %}
  {{ row["?entityName"] }}
  {{ row["?propertyName"] }}
{% endfor %}
\end{lstlisting}

\section{Common Filters}
\label{sec:filters}

Tera provides useful filters for text transformation:

\begin{itemize}
    \item \texttt{lower}: Convert to lowercase
    \item \texttt{upper}: Convert to uppercase
    \item \texttt{snake\_case}: Convert to snake\_case
    \item \texttt{pascal\_case}: Convert to PascalCase
    \item \texttt{unique}: Remove duplicates (by attribute)
\end{itemize}

\begin{lstlisting}[language=HTML, caption={Using filters in templates}]
{{ entity_name | lower }}
{{ entity_name | pascal_case }}
{% for entity in entities | unique(attribute="name") %}
{% endfor %}
\end{lstlisting}

\chapter{RDF Schema Reference}
\label{app:rdf-schema}

\section{Core RDF Vocabulary}
\label{sec:rdf-vocab}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Term} & \textbf{Meaning} \\
\midrule
\texttt{rdf:type} & Indicates class membership \\
\texttt{rdf:Property} & Indicates a property definition \\
\texttt{rdf:value} & Generic value relation \\
\midrule
\texttt{rdfs:Class} & Indicates a class definition \\
\texttt{rdfs:comment} & Documentation string \\
\texttt{rdfs:label} & Human-readable name \\
\texttt{rdfs:domain} & Valid subjects for property \\
\texttt{rdfs:range} & Valid objects for property \\
\texttt{rdfs:subClassOf} & Class hierarchy \\
\texttt{rdfs:subPropertyOf} & Property hierarchy \\
\midrule
\texttt{owl:Class} & Explicit class declaration \\
\texttt{owl:DatatypeProperty} & Property with literal values \\
\texttt{owl:ObjectProperty} & Property with resource values \\
\texttt{owl:FunctionalProperty} & At most one value \\
\bottomrule
\end{tabular}
\caption{Core RDF and OWL vocabulary}
\end{table}

\section{API-Specific Vocabulary}
\label{sec:api-vocab}

This work defines a custom vocabulary for API specifications:

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Property} & \textbf{Meaning} \\
\midrule
\texttt{api:Entity} & API entity/resource type \\
\texttt{api:name} & Entity or property name \\
\texttt{api:type} & Data type (string, integer, etc.) \\
\texttt{api:required} & Property requirement (true/false) \\
\texttt{api:pattern} & Regex pattern constraint \\
\texttt{api:minLength} & Minimum string length \\
\texttt{api:maxLength} & Maximum string length \\
\texttt{api:minimum} & Minimum numeric value \\
\texttt{api:maximum} & Maximum numeric value \\
\texttt{api:format} & Format hint (email, uuid, url, etc.) \\
\texttt{api:Endpoint} & API endpoint definition \\
\texttt{api:path} & HTTP path \\
\texttt{api:method} & HTTP method (GET, POST, etc.) \\
\bottomrule
\end{tabular}
\caption{Custom API vocabulary}
\end{table}

% Bibliography
\newpage
\begin{thebibliography}{99}

\bibitem{rdf-primer} W3C. (2014). RDF 1.1 Primer. Retrieved from https://www.w3.org/TR/rdf11-primer/

\bibitem{sparql-spec} W3C. (2013). SPARQL 1.1 Query Language. Retrieved from https://www.w3.org/TR/sparql11-query/

\bibitem{turtle-spec} W3C. (2014). RDF 1.1 Turtle. Retrieved from https://www.w3.org/TR/turtle/

\bibitem{shacl-spec} W3C. (2017). Shapes Constraint Language (SHACL). Retrieved from https://www.w3.org/TR/shacl/

\bibitem{openapi-spec} OpenAPI Initiative. (2021). OpenAPI Specification 3.1.0. Retrieved from https://spec.openapis.org/oas/latest.html

\bibitem{code-generation} Czarnecki, K., \& Eisenecker, U. W. (2000). Generative programming: methods, tools, and applications. Addison-Wesley.

\bibitem{ontology-patterns} Gangemi, A., \& Presutti, V. (2009). Ontology design patterns. In Handbook on ontologies (pp. 221-243). Springer, Berlin, Heidelberg.

\bibitem{semantic-web} Berners-Lee, T., Hendler, J., \& Lassila, O. (2001). The semantic web. Scientific american, 284(5), 28-37.

\bibitem{description-logics} Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., \& Patel-Schneider, P. F. (Eds.). (2003). The description logic handbook: Theory, implementation, and applications. Cambridge University Press.

\bibitem{api-evolution} Lamothe, J., Taneja, K., \& ($, B. (2017). A systematic mapping study on API evolution. Journal of Software Maintenance and Evolution: Research and Practice, 29(8), e1876.

\end{thebibliography}

\end{document}
