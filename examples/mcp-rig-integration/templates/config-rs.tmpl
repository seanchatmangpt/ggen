---
generator:
  name: "{{ project_name }}_config"
  version: "0.1.0"
  description: "Configuration loader and manager"

variables:
  project_name: "mcp-rig-agent"
  deepseek_support: true
  cohere_support: false
  openai_support: false

output:
  path: "src/config.rs"
---
//! Configuration management
//!
//! This module handles loading and parsing configuration from TOML files.
//! Configuration includes:
//!
//! - AI provider settings (API keys, models)
//! - MCP server definitions (transport, commands)
//! - Application settings (logging, etc.)

use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::Path;

pub mod mcp;

use crate::{McpManager, McpServer, McpTransport};

/// Main configuration structure
#[derive(Debug, Deserialize, Serialize)]
pub struct Config {
    /// AI provider configurations
    pub providers: Providers,

    /// MCP server configurations
    #[serde(default)]
    pub mcp_servers: Vec<mcp::McpServerConfig>,

    /// Application settings
    #[serde(default)]
    pub settings: Settings,
}

impl Config {
    /// Load configuration from a TOML file
    pub fn load<P: AsRef<Path>>(path: P) -> Result<Self> {
        let path = path.as_ref();
        let contents = fs::read_to_string(path)
            .with_context(|| format!("Failed to read config file: {}", path.display()))?;

        let config: Config = toml::from_str(&contents)
            .with_context(|| format!("Failed to parse config file: {}", path.display()))?;

        config.validate()?;

        Ok(config)
    }

    /// Validate the configuration
    fn validate(&self) -> Result<()> {
        // Ensure at least one provider is configured
        if self.providers.deepseek.api_key.is_empty()
            {% if openai_support %}&& self.providers.openai.api_key.is_empty(){% endif %}
            {% if cohere_support %}&& self.providers.cohere.api_key.is_empty(){% endif %} {
            anyhow::bail!("At least one AI provider must be configured with an API key");
        }

        // Validate MCP servers
        for server in &self.mcp_servers {
            server.validate()
                .with_context(|| format!("Invalid MCP server config: {}", server.name))?;
        }

        Ok(())
    }

    /// Create MCP manager from configuration
    pub fn create_mcp_manager(&self) -> Result<McpManager> {
        let mut manager = McpManager::new();

        for server_config in &self.mcp_servers {
            let transport = match &server_config.transport {
                mcp::McpTransportConfig::Stdio { command, args } => {
                    McpTransport::Stdio {
                        command: command.clone(),
                        args: args.clone(),
                    }
                }
                mcp::McpTransportConfig::Sse { url } => {
                    McpTransport::Sse {
                        url: url.clone(),
                    }
                }
                mcp::McpTransportConfig::Streamable { config } => {
                    McpTransport::Streamable {
                        config: config.clone(),
                    }
                }
            };

            let server = McpServer::new(server_config.name.clone(), transport);
            manager.add_server(server);
        }

        Ok(manager)
    }
}

/// AI provider configurations
#[derive(Debug, Deserialize, Serialize)]
pub struct Providers {
    /// DeepSeek configuration
    #[serde(default)]
    pub deepseek: DeepSeekConfig,

    {% if openai_support %}
    /// OpenAI configuration
    #[serde(default)]
    pub openai: OpenAIConfig,
    {% endif %}

    {% if cohere_support %}
    /// Cohere configuration
    #[serde(default)]
    pub cohere: CohereConfig,
    {% endif %}
}

/// DeepSeek provider configuration
#[derive(Debug, Deserialize, Serialize)]
pub struct DeepSeekConfig {
    /// API key (can be overridden by DEEPSEEK_API_KEY env var)
    #[serde(default = "default_deepseek_api_key")]
    pub api_key: String,

    /// Base model identifier
    #[serde(default = "default_deepseek_model")]
    pub model: String,

    /// Chat model name
    #[serde(default = "default_deepseek_chat_model")]
    pub chat_model: String,

    /// Embeddings model name
    #[serde(default = "default_deepseek_embeddings_model")]
    pub embeddings_model: String,
}

impl Default for DeepSeekConfig {
    fn default() -> Self {
        Self {
            api_key: default_deepseek_api_key(),
            model: default_deepseek_model(),
            chat_model: default_deepseek_chat_model(),
            embeddings_model: default_deepseek_embeddings_model(),
        }
    }
}

fn default_deepseek_api_key() -> String {
    std::env::var("DEEPSEEK_API_KEY").unwrap_or_default()
}

fn default_deepseek_model() -> String {
    "deepseek-chat".to_string()
}

fn default_deepseek_chat_model() -> String {
    "deepseek-chat".to_string()
}

fn default_deepseek_embeddings_model() -> String {
    "deepseek-embedding".to_string()
}

{% if openai_support %}
/// OpenAI provider configuration
#[derive(Debug, Deserialize, Serialize)]
pub struct OpenAIConfig {
    #[serde(default = "default_openai_api_key")]
    pub api_key: String,

    #[serde(default = "default_openai_model")]
    pub model: String,
}

impl Default for OpenAIConfig {
    fn default() -> Self {
        Self {
            api_key: default_openai_api_key(),
            model: default_openai_model(),
        }
    }
}

fn default_openai_api_key() -> String {
    std::env::var("OPENAI_API_KEY").unwrap_or_default()
}

fn default_openai_model() -> String {
    "gpt-4-turbo-preview".to_string()
}
{% endif %}

{% if cohere_support %}
/// Cohere provider configuration
#[derive(Debug, Deserialize, Serialize)]
pub struct CohereConfig {
    #[serde(default = "default_cohere_api_key")]
    pub api_key: String,

    #[serde(default = "default_cohere_model")]
    pub model: String,
}

impl Default for CohereConfig {
    fn default() -> Self {
        Self {
            api_key: default_cohere_api_key(),
            model: default_cohere_model(),
        }
    }
}

fn default_cohere_api_key() -> String {
    std::env::var("COHERE_API_KEY").unwrap_or_default()
}

fn default_cohere_model() -> String {
    "command-r-plus".to_string()
}
{% endif %}

/// Application settings
#[derive(Debug, Deserialize, Serialize)]
pub struct Settings {
    /// Directory for logs
    #[serde(default = "default_log_dir")]
    pub log_dir: String,

    /// Enable verbose logging
    #[serde(default)]
    pub verbose: bool,
}

impl Default for Settings {
    fn default() -> Self {
        Self {
            log_dir: default_log_dir(),
            verbose: false,
        }
    }
}

fn default_log_dir() -> String {
    "logs".to_string()
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_config_defaults() {
        let config = Config {
            providers: Providers {
                deepseek: DeepSeekConfig::default(),
                {% if openai_support %}openai: OpenAIConfig::default(),{% endif %}
                {% if cohere_support %}cohere: CohereConfig::default(),{% endif %}
            },
            mcp_servers: vec![],
            settings: Settings::default(),
        };

        assert_eq!(config.settings.log_dir, "logs");
        assert!(!config.settings.verbose);
    }

    #[test]
    fn test_toml_parsing() {
        let toml = r#"
            [providers.deepseek]
            api_key = "test-key"
            model = "deepseek-chat"

            [settings]
            log_dir = "test_logs"
            verbose = true
        "#;

        let config: Config = toml::from_str(toml).unwrap();
        assert_eq!(config.providers.deepseek.api_key, "test-key");
        assert_eq!(config.settings.log_dir, "test_logs");
        assert!(config.settings.verbose);
    }
}
