//! Simple test for Ollama client with qwen3-coder:30b

use ggen_ai::providers::OllamaClient;
use ggen_ai::client::LlmClient;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Warn if running example without explicit confirmation
    if std::env::var("GGEN_ALLOW_LIVE_CALLS").is_err() {
        eprintln!("‚ö†Ô∏è  This example makes live API calls to Ollama.");
        eprintln!("Set GGEN_ALLOW_LIVE_CALLS=1 to proceed.");
        eprintln!("Or use: GGEN_ALLOW_LIVE_CALLS=1 cargo run --example test-ollama");
        return Ok(());
    }
    
    println!("üß™ Testing Ollama client with qwen3-coder:30b");
    println!("=============================================");

    let client = OllamaClient::new();
    
    // Test basic completion
    println!("üìù Testing basic completion...");
    let response = client.complete(
        "Generate a simple Rust struct for a User with id, name, and email fields",
        None
    ).await?;
    
    println!("‚úÖ Response received:");
    println!("Model: {}", response.model);
    println!("Content: {}", response.content);
    
    if let Some(usage) = response.usage {
        println!("Usage: {} prompt tokens, {} completion tokens", 
                usage.prompt_tokens, usage.completion_tokens);
    }

    // Test with custom config
    println!("\nüîß Testing with qwen3-coder:30b config...");
    let config = OllamaClient::qwen3_coder_config();
    let response2 = client.complete(
        "Create a TypeScript interface for a Product with name, price, and category",
        Some(config)
    ).await?;
    
    println!("‚úÖ Response received:");
    println!("Model: {}", response2.model);
    println!("Content: {}", response2.content);

    println!("\nüéâ Ollama client test completed successfully!");

    Ok(())
}

